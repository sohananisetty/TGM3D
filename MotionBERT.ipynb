{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c27919-3c6a-4223-8728-24fcd8a22e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabfd305-59ca-4d5f-b319-b7151f135a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7346704e-c584-4431-8c28-5282e76c8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f82299-7ba4-4229-b3da-7b1ee47f358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9110ff05-8603-4916-83a3-909926251190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import get_cfg_defaults\n",
    "from core.datasets.dataset_loading_utils import load_dataset\n",
    "from core.datasets.vq_dataset import DATALoader\n",
    "from utils.vis_utils import plot_3d_global\n",
    "from core.models.conformer_vqvae import ConformerVQMotionModel, Encoder\n",
    "from torch.utils import data\n",
    "from core.datasets.vq_dataset import DATALoader, MotionCollator\n",
    "from einops import pack, rearrange, reduce, repeat, unpack\n",
    "\n",
    "def pack_one(t, pattern):\n",
    "    return pack([t], pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7569282a-d096-439a-afc8-1a3184229695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.vis_utils.plot_3d_global as plot_3d\n",
    "from utils.motion_processing.hml_process import recover_from_ric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887cb036-5619-469a-b735-8d4e495c97f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a3688bb-c984-43bf-ade8-f55d8047318c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e790c066-3169-472f-b9e2-9d4773b86569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vis_utils.render_final import Renderer\n",
    "renderer = Renderer(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af892c6-0233-47a0-9909-32412a4af3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from: /srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_768_1024_hmlvec/conformer_768_1024_hmlvec.yaml\n",
      "tensor([200000.])\n"
     ]
    }
   ],
   "source": [
    "path = \"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_768_1024_hmlvec/conformer_768_1024_hmlvec.yaml\"\n",
    "cfg = get_cfg_defaults()\n",
    "print(\"loading config from:\", path)\n",
    "cfg.merge_from_file(path)\n",
    "cfg.freeze()\n",
    "\n",
    "ckpt = torch.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_768_1024_hmlvec/vqvae_motion.pt\" , map_location=\"cpu\")\n",
    "print(ckpt[\"steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5ba701-a4a9-4e72-8158-4fee24ab019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d00b75-fac9-4dda-8706-d15b5c8f7769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['core.models.conformer_vqvae', 'ConformerVQMotionModel']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"core.models.conformer_vqvae.ConformerVQMotionModel\").rsplit(\".\" , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de96f701-5a8d-433d-b688-5b9302cf4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(importlib.import_module('core.models.conformer_vqvae'), 'ConformerVQMotionModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ca2a605-2846-4dce-a8ff-f248cb7d61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync is turned on False\n"
     ]
    }
   ],
   "source": [
    "convvq = model(cfg.vqvae).to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "782bde2e-f8b9-4d44-8a92-01c6913ee68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync is turned on False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from core.models.conformer_vqvae import ConformerVQMotionModel, Encoder\n",
    "convvq = ConformerVQMotionModel(cfg.vqvae).to(device).eval()\n",
    "convvq.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_768_1024_hmlvec/vqvae_motion.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbc534c7-3e84-47da-b67a-d3ee3d129cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ../motion_vqvae/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a942e0-2637-425d-8048-0242c26472a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from configs.config import get_cfg_defaults\n",
    "\n",
    "# path = \"/srv/hays-lab/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/conv_vq/convq_512_512/convq_512_512.yaml\"\n",
    "# cfg = get_cfg_defaults()\n",
    "# print(\"loading config from:\", path)\n",
    "# cfg.merge_from_file(path)\n",
    "# cfg.freeze()\n",
    "\n",
    "# ckpt = torch.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/conv_vq/convq_512_512/vqvae_motion.pt\" , map_location=\"cpu\")\n",
    "# print(ckpt[\"steps\"])\n",
    "\n",
    "# from motion_vqvae.core.models.conv_vqvae import ConvVQMotionModel\n",
    "# convvq = ConvVQMotionModel(cfg.vqvae).to(device).eval()\n",
    "\n",
    "# convvq.load_state_dict(ckpt[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbd7d261-56a2-4eee-a26e-349d7cd5ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "alll = glob(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/AIST/new_joint_vecs/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540078a7-9ac3-4036-a198-5ad87730f5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2279191-e562-4415-b457-32010c41368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/AIST_SMPL/all.txt\" , \"w\") as f:\n",
    "    for line in alll:\n",
    "        f.write(f'{line.split(\"/\")[-1].split(\".\")[0]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5db0c61-4447-4fd1-b7df-708828aaa2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vq_dataset import VQMotionDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a0544fc-47d1-41a9-a2f1-39853636e69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4384/4384 [00:02<00:00, 1781.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 4384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQMotionDataset(\"t2m\" , \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion\" , window_size = -1, split = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d04642-c1b3-4dd1-81e9-85a501297d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f90ff4ef-c578-4059-b32d-f9b92541444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DATALoader(\n",
    "            train_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            collate_fn=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b1f28f4-85d3-47d5-8f21-52db9c34b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dl:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ee5717e-4508-4a34-901a-f8e8672664b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest = \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionIndices/HumanML3D/joint_indices\"\n",
    "os.makedirs(dest, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5a758-e991-43b2-a9bc-3793c7beaa84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e379da16-09be-4606-abb0-2ac0cb3d8efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2726/2726 [02:14<00:00, 20.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(train_dl)):\n",
    "    # if i < 12300:\n",
    "    #     continue\n",
    "    \n",
    "    gt_motion = batch[\"motion\"].to(device)\n",
    "    if gt_motion.shape[1] > 100:\n",
    "        ind = []\n",
    "        for m in range(0, gt_motion.shape[1], 100):\n",
    "            indics = convvq.encode(gt_motion[:, m:m+100])\n",
    "            ind.append(indics[0])\n",
    "        indices = torch.cat(ind)[None]\n",
    "    else:\n",
    "        indices = convvq.encode(gt_motion)\n",
    "    np.save(os.path.join(dest , batch[\"names\"][0]+\".npy\") , indices.detach().cpu().numpy())\n",
    "    del indices\n",
    "    del gt_motion\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d08a12-9145-4b6f-b694-70508fafec17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a27f53-1229-434f-961a-07d0aaf4115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPLIndices/AIST/joint_indices/M_gJS_sBM_cAll_d03_mJS3_ch02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6c717-21f4-4650-a55d-dbe7e97ecf52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91969d6-e88d-47f2-aae2-ae55f42e306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "og = f\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/HumanML3D/new_joint_vecs/{batch['names'][0]}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d666be5-1052-4fe7-be3a-fb86d1fd85b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 199, 263])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dl:\n",
    "    break\n",
    "gt_motion = batch[\"motion\"][:,:1000]\n",
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "700c4514-feac-4201-bf31-c11c3940cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gt_motion.shape[1] > 200:\n",
    "    ind = []\n",
    "    for m in range(0, gt_motion.shape[1], 80):\n",
    "        indics = convvq.encode(gt_motion[:, m:m+80].to(device))\n",
    "        ind.append(indics[0])\n",
    "    indices = torch.cat(ind)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac98f604-e528-49a2-a224-bbc3c09bb716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = convvq.encode(gt_motion.to(device))\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c9abed9-59ae-4058-b48f-148f7de17fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 304,    1,  587,  411,  889,    0,    0,  532,  778,  616,  587,  529,\n",
       "          300,  300,  927, 1020,  721,  402,  995,  389,  801,  147,   41,  870,\n",
       "          995,  995,  763,  310,  147,   41,  922,  411,  347,  616,  389,  332,\n",
       "          217,  411,  704,  995,  402,  332,  821,  153,  393,  347,  274,  680,\n",
       "          644,  992]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a30a9c6-cb17-4030-8c82-c0c0d66316d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 263])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized, decoded_motion_features = convvq.decode(indices.long())\n",
    "decoded_motion_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e2923de-a6bb-497b-ab17-ee45125c2630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f54e14dd-554c-4bfc-a4b9-bec8496d939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion1 = (\n",
    "    train_ds.inv_transform(gt_motion.cpu())\n",
    "    .squeeze()\n",
    "    .float()\n",
    ")\n",
    "pred_motion = (\n",
    "    train_ds.inv_transform(decoded_motion_features.cpu())\n",
    "    .squeeze()\n",
    "    .float()\n",
    ")\n",
    "\n",
    "save_file = \"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/renders\"\n",
    "gt_motion_xyz = recover_from_ric(gt_motion1, 22)\n",
    "pred_motion_xyz = recover_from_ric(pred_motion, 22)\n",
    "\n",
    "gt_pose_vis = plot_3d.draw_to_batch(\n",
    "    gt_motion_xyz.numpy().squeeze()[None],\n",
    "    None,\n",
    "    [os.path.join(save_file, \"t\" + \"_gt.gif\")],\n",
    ")\n",
    "pred_pose_vis = plot_3d.draw_to_batch(\n",
    "    pred_motion_xyz.numpy().squeeze()[None],\n",
    "    None,\n",
    "    [os.path.join(save_file, \"t\" + \"_pred.gif\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3320e-1e06-45fe-87b8-044d9d74e4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56c066d3-cd58-47ae-afa0-66e71517022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from core.datasets import dataset_TM_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b4512-71c3-45b7-bc24-ad36aece8c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93364710-fcda-4a8b-9ad5-631c8dcddd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4384/4384 [00:04<00:00, 1066.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4248 4248\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w_vectorizer = WordVectorizer(\n",
    "   \"/srv/hays-lab/scratch/sanisetty3/music_motion/T2M-GPT/glove\", \"our_vab\"\n",
    ")\n",
    "eval_wrapper = EvaluatorModelWrapper(cfg.eval_model)\n",
    "tm_eval = dataset_TM_eval.DATALoader(\n",
    "    32,\n",
    "    w_vectorizer,\n",
    "    unit_length=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a365f4d9-e219-411e-b2fd-385119434eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_trans import calculate_R_precision, calculate_multimodality, calculate_diversity, calculate_frechet_distance, calculate_activation_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ddcd6-1f68-4909-8bb2-707c0533be2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "afe3d6cd-c3b4-4b34-bdfb-3c2810b66def",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer.render(\n",
    "    motion_vec=ogg[:1000,:135],\n",
    "    outdir=\"./renders/\",\n",
    "    step=0,\n",
    "    name=f\"00_000007_og\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8092f2-8f5f-440d-9eb8-bcbe1f9b29a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94880c46-de79-4e1d-9ff0-db7163a10faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb15fcc-f4c4-4fd2-a88e-352526e906f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6bd0c2e-bec7-47bc-b115-5c59078ebfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25bef9be-44eb-44c9-992f-6a2356f5b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hml = \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/HumanML3D/Mean.npy\"\n",
    "hml2 = \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/AIST/Mean.npy\"\n",
    "hml3 = \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/Choreomaster/Mean.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "268210f1-5360-4eb4-b256-069a8bf897c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/Mean.npy\" , np.mean([np.load(hml) + np.load(hml2) + np.load(hml3)] , 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d869b8-f7d6-47dd-889f-af852f8f22bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c71ac0fc-62aa-4b5f-ad6a-7867c1a7d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "db0eccd4-791d-4386-b107-bcff752f6331",
   "metadata": {},
   "outputs": [],
   "source": [
    "pths = sorted(glob(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/HumanML3D/new_joint_vecs/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aecd03dc-f424-4e79-841b-d505cf7c5443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/HumanML3D/new_joint_vecs/M008676.npy'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pths[25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "47310a88-0986-4cde-b532-e7ea13b1db5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8676"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(pths[25000].split(\"/\")[-1].split(\".\")[0][-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8eff3e1e-adb4-42fa-bd00-4cf93073bad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 32648/32648 [00:00<00:00, 964613.99it/s]\n"
     ]
    }
   ],
   "source": [
    "add = []\n",
    "for p in tqdm(pths):\n",
    "    nm = int(p.split(\"/\")[-1].split(\".\")[0][-6:])\n",
    "    if nm > 14616:\n",
    "        add.append(p.split(\"/\")[-1].split(\".\")[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "59791185-879b-4e1e-b45c-85be057e000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open(r'/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/HumanML3D/train.txt', 'a') as fp:\n",
    "    for item in add:\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5b9d3c0b-51e0-4b6a-aabb-533b387cee42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3418"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "546060ca-56b9-40dc-93a8-d57fd9a43d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 32648/32648 [00:00<00:00, 1130149.03it/s]\n"
     ]
    }
   ],
   "source": [
    "add = []\n",
    "for p in tqdm(pths):\n",
    "    n = p.split(\"/\")[-1].split(\".\")[0]\n",
    "    add.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2ac9da65-d194-4b66-b00a-84ad44969eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9329c4-7e76-496b-b0a5-2fa740078a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce3ff326-0237-48d3-bf91-0490039d8d65",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65831f99-a68e-49ae-9a85-7958f2de0aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share2/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from core.datasets.dataset_loading_utils import load_dataset_bert\n",
    "from core.datasets.motion_bert_dataset import BERTMotionDataset, DATALoader\n",
    "from core.models.BERT import BERT, BERTParams\n",
    "from core.optimizer import get_optimizer\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0cb4b2-cc2e-4bf4-b19d-be127f5889a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d65d85e2-6d32-4a2d-860e-ee2aad75baf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from: /srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/bert_12_768/bert_12_768.yaml\n"
     ]
    }
   ],
   "source": [
    "path = \"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/bert_12_768/bert_12_768.yaml\"\n",
    "cfg = get_cfg_defaults()\n",
    "print(\"loading config from:\", path)\n",
    "cfg.merge_from_file(path)\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06bd026a-4b67-4ee0-80d0-492cd2cde41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4384/4384 [00:02<00:00, 2117.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 4376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds, sampler_train, weights_train = load_dataset_bert(\n",
    "                dataset_names=[\"t2m\"],\n",
    "                args=cfg,\n",
    "                split=\"test\",\n",
    "                weight_scale=[1],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6a720e4-9f65-4887-b8cb-3bd45c90deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DATALoader(\n",
    "            train_ds,\n",
    "            batch_size=20,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35128692-ae71-4697-97e1-2214c7c55497",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a1703-1020-4bcc-b581-11c4499410d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cf37d-76e6-41f9-a199-4985f20a8b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a166278-20a9-4a5c-b76e-c5410d6ba7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79513e86-4d78-464f-8402-701490815c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.pretrained_embedding = torch.nn.Embedding.from_pretrained(glove_embeddings)\n",
    "self.trainable_embedding = torch.nn.Embedding(\n",
    "    how_many_tokens_not_present, glove_embeddings.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bc573dc-9274-4089-8af3-af2d5c5fa677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1024, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Embedding.from_pretrained(convvq.vq.codebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee9c44c7-1fcf-4a4f-9139-7bfb6b34c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = BERTParams()\n",
    "bert = BERT(params).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "245be0ce-79b3-4f8c-a139-f367b7984945",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [False , True]\n",
    "a.extend([True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36a662d3-1846-45ee-ab24-f86f717fb36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(convvq.vq.codebook , \"./checkpoints/conformer_768_1024_hmlvec/codebook.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42024b0-deac-4bfd-b934-1113e66f1049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b639e-f86c-43a6-9ee3-784d56de0b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2486574d-a12f-461b-941f-d6a5f1f6b5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1026, 1024,  611,  242, 1024,  845,  193,  591,  888, 1024, 1024,  699,\n",
       "          153,  193,  822,  591,  197,  699,  845,  360,   33, 1024, 1024,  358,\n",
       "           33, 1024, 1024,  142,  591,  197,  699, 1024,  360,  591, 1024,  897,\n",
       "         1024,  699,  447,  193,  177, 1024, 1024,   74,  148,  999,  674,  142,\n",
       "          552,  627,  699, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025],\n",
       "        [1026,  565, 1024,  565,  692,  692, 1024,  129, 1011,  482,  632,  791,\n",
       "          288,   52,  204, 1024,  241, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"bert_input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "582b1a88-f36c-4903-b3e5-3f48a5ff5a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = data[\"bert_input\"] != 1024\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef22bd4f-8b77-4d04-b0ab-fce38278b86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.],\n",
       "        [2.],\n",
       "        [8.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.randint(0,10 , (5 , 1)).float()\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38436ab5-7621-4622-904b-208962f0fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(128)\n",
    "mask[:p] = 0\n",
    "np.random.shuffle(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67958bb5-e305-4dbe-a28c-ef502b11fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.nn.Embedding(1027 , 768 , padding_idx=1025)\n",
    "e.weight.data[:1024] = convvq.vq.codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eb74bf5-2866-466d-ac70-ecde4b515861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(convvq.vq.codebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0fd5206c-4039-46ab-ada8-4b3f168ecdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(10).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2ff70f77-10ba-4b6a-9587-ac03ce3e1c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 768])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convvq.vq.codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "049998dc-26d7-43e9-b5fa-445157892e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.weight.data[:1024] = convvq.vq.codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c3eee42f-06b8-4ec5-b46e-ea755528ac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 7., 1., 9., 8.],\n",
       "        [5., 9., 1., 5., 3.],\n",
       "        [3., 5., 2., 3., 2.],\n",
       "        [3., 8., 9., 7., 2.],\n",
       "        [2., 5., 7., 5., 1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dots= torch.randint(0,10 , (5,5)).float()\n",
    "dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e845d433-b958-4afa-8f19-f1a1f9ea8b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.Tensor([False,True,False,False,True]).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9e264788-c8ab-4edb-91ad-ade203c2ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = dots.masked_fill(~mask, -torch.finfo(dots.dtype).max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90b29f73-9b64-4e36-b9ec-389bd7391bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4028e+38,  7.0000e+00, -3.4028e+38, -3.4028e+38,  8.0000e+00],\n",
       "        [-3.4028e+38,  9.0000e+00, -3.4028e+38, -3.4028e+38,  3.0000e+00],\n",
       "        [-3.4028e+38,  5.0000e+00, -3.4028e+38, -3.4028e+38,  2.0000e+00],\n",
       "        [-3.4028e+38,  8.0000e+00, -3.4028e+38, -3.4028e+38,  2.0000e+00],\n",
       "        [-3.4028e+38,  5.0000e+00, -3.4028e+38, -3.4028e+38,  1.0000e+00]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "befb1f0e-c692-4b71-ae07-c8816697568f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7311],\n",
       "        [2.0025],\n",
       "        [2.0474],\n",
       "        [2.0025],\n",
       "        [2.0180]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(dots , -1)@v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a563d69-f41f-4745-ad7c-e375d84f9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_loss_fnc = torch.nn.CrossEntropyLoss(\n",
    "            ignore_index=bert.pad_index\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "622e8c75-4093-4db0-bdc2-7243b3880245",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_lm_output = bert.forward(data[\"bert_input\"].cuda())\n",
    "\n",
    "mask_loss = mlm_loss_fnc(\n",
    "    mask_lm_output.transpose(1, 2), data[\"bert_label\"].cuda()\n",
    ")\n",
    "loss = cfg.bert.loss_mlm * mask_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b83d1858-f639-409a-bfb9-6095a16337d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"bert_input\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a208a00c-a983-40c0-875b-3327689d71bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1025, 1025, 1025,  242, 1025, 1025,  193, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025,  922, 1025,  197, 1025,  845,  360, 1025, 1025,  552, 1025,\n",
       "          148, 1025, 1025,  142, 1025, 1025,  699, 1025, 1025,  591, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,  999, 1025,  142,\n",
       "         1025, 1025,  699, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025],\n",
       "        [1025, 1025, 1025, 1025, 1025, 1025, 1025,  129, 1025, 1025,  632, 1025,\n",
       "         1025,   52, 1025,   52, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025,\n",
       "         1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"bert_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "134e8ccb-b5f9-4933-ae98-9b3ea573d91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 1028])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_lm_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1072191-7135-4e1f-be58-077fe940f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm = torch.nn.functional.softmax(mask_lm_output , -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd481d5c-ab33-4fe2-bd59-9057506818e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0028, 0.0008, 0.0013,  ..., 0.0005, 0.0005, 0.0004], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5283c-5df1-498d-8113-f8b7608dbd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5d77772-4de2-4683-bdd0-c40b5953a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = data[\"bert_label\"]!=1025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "02f32f60-a23d-4a41-9420-a3af0c781ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([242, 193, 922, 197, 845, 360, 552, 148, 142, 699, 591, 999, 142, 699,\n",
       "        129, 632,  52,  52])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"bert_label\"][msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb480ae8-5b19-4ead-9ce9-b6fe69aee9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = mlm.argmax(-1).cpu()[msk]  == data[\"bert_label\"][msk]\n",
    "correct = correct.sum()/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b99cbd8-19d1-4d20-a015-26523c091294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d897039-c34c-4266-ad33-5a7bde995103",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_loss_fnc = torch.nn.NLLLoss(ignore_index=0)\n",
    "mlm_loss_fnc = torch.nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0766820c-ee43-4ce1-91a3-089ddfcd550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_lm_output = bert.forward(data[\"bert_input\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63d119bc-63b3-4434-be21-a87dd15e3962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1028, 768, padding_idx=1025)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.token_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8238d8c7-60ce-4583-8a52-777555f3903e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "116f2b69-5ee4-4d21-8dd4-2f8522fd7ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "135 + 3*22 + 22*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "769b9d8f-fc73-41fc-9616-f12ebe6b36fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "135 + 3*22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5083ce89-4df6-4efc-97d9-a17cfb70c3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "22*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea44130c-b80f-41cd-a2c1-050e5a535208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_sent_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7247598c-e113-4c6d-bdd7-951ca029f99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"is_next\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4108e18e-1ce7-453a-84a1-368bf1b3b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_loss = nsp_loss_fnc(next_sent_output, data[\"is_next\"].cuda().reshape(-1))\n",
    "\n",
    "# 2-2. NLLLoss of predicting masked token word\n",
    "mask_loss = mlm_loss_fnc(\n",
    "    mask_lm_output.transpose(1, 2), data[\"bert_label\"].cuda()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9d8c729-66f6-4cd8-a415-c7f50a577b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.0331, device='cuda:0', grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d16b9344-eb7a-4a42-b560-ac6680b00350",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = (\n",
    "                    next_sent_output.argmax(dim=-1).eq(data[\"is_next\"].cuda()).sum().item()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d638cccd-b3f0-4b89-aa7c-4f46eced4aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_sent_output.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "363b0b30-38e1-4a3d-8b24-b9b88de3bdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"is_next\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72bea50d-96ab-4d70-ba24-02a205fddcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42af97d5-dd9f-40cb-845e-765ab60081c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "    mask_excess = mask.cumsum(dim=-1) > (num_tokens * prob).ceil()\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    return new_mask[:, 1:].bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d01a1848-c059-44b6-8d2a-337307d71be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baad915e-1884-475b-bf47-f859abcd1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ignore_token_ids = set([1025, 1026, 1027])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "861125b2-bb3c-441e-a1a5-052aaf65e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.Tensor([1026] + list(np.random.randint(0, 1024 , (10))) + [1027]).long()[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24020413-60dd-4106-afb2-06a510510726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f468f471-bfe9-4f7e-b4e4-5a48d44626a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_mask = mask_with_tokens(seq, mask_ignore_token_ids)\n",
    "mask = get_mask_subset_with_prob(~no_mask, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e7721fd-f87c-4579-bf3e-ae3a72f67dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True, False,  True, False,  True,  True, False,\n",
       "         False, False]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe2d3730-3684-427f-9c95-7e56813d1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = seq.masked_fill(~mask, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4b5a0797-6ead-4db7-8dcf-a8cefdad0412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1024, 1024, 1024,   90, 1024,  310, 1024,  914,  344, 1024, 1024, 1024]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c463e8-48a8-48cc-9a4c-fc7f4e17fd8d",
   "metadata": {},
   "source": [
    "## text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab7b05f-524f-42b7-959e-b44e860078dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hml_txs = \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/HumanML3D/texts/000000.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e28981e3-0b9c-41cd-bf8c-7387c35d16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import codecs as cs\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "def get_caption(path):\n",
    "    text_data = []\n",
    "    captions = []\n",
    "    flag = False\n",
    "    with cs.open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            text_dict = {}\n",
    "            line_split = line.strip().split('#')\n",
    "            caption = line_split[0]\n",
    "            captions.append(caption)\n",
    "            tokens = line_split[1].split(' ')\n",
    "            f_tag = float(line_split[2])\n",
    "            to_tag = float(line_split[3])\n",
    "            f_tag = 0.0 if np.isnan(f_tag) else f_tag\n",
    "            to_tag = 0.0 if np.isnan(to_tag) else to_tag\n",
    "    \n",
    "            text_dict['caption'] = caption\n",
    "            text_dict['tokens'] = tokens\n",
    "            if f_tag == 0.0 and to_tag == 0.0:\n",
    "                flag = True\n",
    "                text_data.append(text_dict)\n",
    "            else:\n",
    "                try:\n",
    "                    n_motion = motion[int(f_tag*fps) : int(to_tag*fps)]\n",
    "                    if (len(n_motion)) < min_motion_len or (len(n_motion) >= 200):\n",
    "                        continue\n",
    "                    new_name = random.choice('ABCDEFGHIJKLMNOPQRSTUVW') + '_' + name\n",
    "                    while new_name in data_dict:\n",
    "                        new_name = random.choice('ABCDEFGHIJKLMNOPQRSTUVW') + '_' + name\n",
    "                    data_dict[new_name] = {'motion': n_motion,\n",
    "                                           'length': len(n_motion),\n",
    "                                           'text':[text_dict]}\n",
    "                    new_name_list.append(new_name)\n",
    "                    length_list.append(len(n_motion))\n",
    "                except:\n",
    "                    print(line_split)\n",
    "                    print(line_split[2], line_split[3], f_tag, to_tag, name)\n",
    "                    # break\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bbc64fc-a5e6-40fc-97b9-956509799518",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = get_caption(hml_txs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3acea43a-93c7-46a6-8540-4a61824846a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a man kicks something or someone with his left leg.',\n",
       " 'the standing person kicks with their left foot before going back to their original stance.',\n",
       " 'a man kicks with something or someone with his left leg.',\n",
       " 'he is flying kick with his left leg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983361bd-223b-49b8-81e4-68791a88f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = T5(128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
