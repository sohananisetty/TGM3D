{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c210f3-99ba-40e8-bf38-3373f13bca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1217d9bc-fc3e-492a-b216-2ee9fb7086e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b096c1d9-386e-4b0e-ab6f-f2daf9b1a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator\n",
    "from einops import rearrange, reduce, pack, unpack\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debdf11b-1fc3-4e7e-b273-f68ca059a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e057cc3-7400-46b9-8dc9-9e5c45b1a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.conv_vqvae import ConvVQMotionModel\n",
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.conformer_vqvae import ConformerVQMotionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "a4e5ba5c-201b-4b70-8443-25244ae22258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from: /srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_512_1024_affine/conformer_512_1024_affine.yaml\n"
     ]
    }
   ],
   "source": [
    "path = \"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_512_1024_affine/conformer_512_1024_affine.yaml\"\n",
    "cfg = get_cfg_defaults()\n",
    "print(\"loading config from:\", path)\n",
    "cfg.merge_from_file(path)\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2076150b-628e-47a3-be4c-6883b3631f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync is turned on False\n"
     ]
    }
   ],
   "source": [
    "vqvae_model = ConformerVQMotionModel(\n",
    "    cfg.vqvae,\n",
    "    # is_distributed=self.is_distributed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03c02e84-e74d-4584-9b6e-2fdb79450650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training params: 16.58M\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in vqvae_model.motionEncoder.parameters() if p.requires_grad)\n",
    "print(\"Total training params: %.2fM\" % (total / 1e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc816d7-e1be-4691-b2a4-ea161c98a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1,80,271))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a842909-8cc4-493b-82df-ed88e80ec12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, ind, lss = vqvae_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de82ac13-7ed7-49c8-a7d7-808428b598c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 271])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab8b076b-e029-41a4-aac3-283bd8c43749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.vqvae.depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c15abb-1163-4b93-b2c3-8249fb18f8f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Calc mean comman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9f407c-703f-4cbc-8ea2-49126fc8b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "hml = \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D_SMPL\"\n",
    "aist = \"/srv/hays-lab/scratch/sanisetty3/music_motion/AIST_SMPL\"\n",
    "cm =\"/srv/hays-lab/scratch/sanisetty3/music_motion/Choreomaster_SMPL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e23a0d-7dab-4eeb-88d6-8c71cee1a9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76ebff3a-6191-4726-88e7-026e7896bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = np.load(os.path.join(hml, \"Mean.npy\"))\n",
    "ma = np.load(os.path.join(aist, \"Mean.npy\"))\n",
    "mc = np.load(os.path.join(cm, \"Mean.npy\"))\n",
    "\n",
    "sh = np.load(os.path.join(hml, \"Std.npy\"))\n",
    "sa = np.load(os.path.join(aist, \"Std.npy\"))\n",
    "sc = np.load(os.path.join(cm, \"Std.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adb0af74-8977-4f6b-8da7-332720224716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b310fc4d-1af5-49b6-89ee-364f1e576568",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.mean([mh, ma,mc] , 0)\n",
    "s = np.mean([sh,sa,sc] , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fab7e4d4-0a19-43a4-aa7f-9e83cd2dfe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/pretrained/common/Std.npy\" , s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27211f97-c964-438a-8087-0c99f740ea04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ab72500-b7a3-4907-b9d9-acff965778e1",
   "metadata": {},
   "source": [
    "## Muse stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "feaf7d2b-0414-4181-a99e-dfebccbee802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32767b3e-6018-43e0-9a04-f6591166b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AttentionParams:\n",
    "    dim: int = 768\n",
    "    dim_head: int = 96\n",
    "    heads: int = 8\n",
    "    causal: bool = False\n",
    "    qk_norm: bool = False\n",
    "    qk_norm_scale: int = 8\n",
    "    dropout: float = 0.0\n",
    "    cross_attn_tokens_dropout: float = (0.0,)\n",
    "    add_null_kv: bool = False\n",
    "    flash: bool = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08318807-b406-41de-8da0-49bd0f0ad154",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AttentionParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfbd23a0-b464-4a69-8a50-128d6c307af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.causal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9531a2c0-a035-465d-8317-263987888a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class PositionalEmbeddingType(Enum):\n",
    "    REL= \"RelativePositionBias\"\n",
    "    SINE= \"ScaledSinusoidalEmbedding\"\n",
    "    ALIBI= \"AlibiPositionalBias\"\n",
    "    ABS= \"AbsolutePositionalEmbedding\"\n",
    "    SHAW= \"ShawRelativePositionalEmbedding\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "167c7a17-8992-4352-b4dd-0055c70a4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PositionalEmbeddingType.REL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f71415fa-a1a7-4871-bfc7-38a91b882f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((4,100,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c5152-67cd-4108-b259-351d88d44538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d481b5ca-759d-47f3-b027-2e154c51618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = nn.InstanceNorm1d(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c95a016-c267-4d19-bd25-d12924754f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(shape, min=0, max=1, device=None):\n",
    "    return torch.zeros(shape, device=device).float().uniform_(0, 1)\n",
    "\n",
    "\n",
    "def prob_mask_like(shape, prob, device=None):\n",
    "    if prob == 1:\n",
    "        return torch.ones(shape, device=device, dtype=torch.bool)\n",
    "    elif prob == 0:\n",
    "        return torch.zeros(shape, device=device, dtype=torch.bool)\n",
    "    else:\n",
    "        return uniform(shape, device=device) < prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4cc56ccf-ab2b-466f-9e09-d5683d911036",
   "metadata": {},
   "outputs": [],
   "source": [
    "b,n,d = a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61884ec7-39ff-462c-9274-b04105ffbe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ = prob_mask_like((b, 1), 1.0 - 0.5, \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5059994-9e58-4b05-9321-0bb41d86c74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c6c4acb-4280-4f65-8170-a37b08e25c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_mask = (a != 1026).any(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832189d-a1f1-4efe-a527-0d5a3f36176a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1522c7f-ecf5-4e7c-b36c-b3f6aae0654f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72722d35-207c-4118-9ccd-5321d33ddbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_mask = context_mask & mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c86326fd-ebaf-4c10-9e47-fe84919184c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share2/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Config, T5EncoderModel, T5Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b45b176-be4b-4a7e-88f2-fd01e44d2586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|███████████████████████████████████████████████████████████████████| 605/605 [00:00<00:00, 81.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "config = T5Config.from_pretrained(\"google/t5-v1_1-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "8c5d11f4-4069-4db9-b323-e87cd3aa1813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "15ca9ffa-7d12-44f9-a808-3cf79739fa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23d106-55dd-4f38-8ec5-723365a7d41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092dd7a-d681-44b1-a11b-13072a93815a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f810279-f0ad-4cdd-b289-027dd0adfa75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee9403-b5e3-40f6-8487-df608d49a733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cdc023-2a6b-4901-87c3-0a0895a18899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "8246dcc7-60ab-4350-a502-08d7bc06beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.motion_clip.motion_clip import MotionClipEncoder\n",
    "\n",
    "ckpt = torch.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/core/models/motion_clip/MotionClipEncoder.pt\", map_location=\"cpu\")\n",
    "enc = MotionClipEncoder()\n",
    "enc.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "dd0270d6-1c26-40b9-859f-7e7e372e661c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953ec3d-1255-4b52-b64b-503d46b8ca71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9b6f704-25a3-4625-ac90-0a0a186cc80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.zeros(10, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bf81601-3abc-4477-af31-1640e216da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23373304-9ec6-4fd7-bf1c-30b064caff01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31433dc-c4de-4728-867c-71d3e8ec7830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df582b-d028-4ff0-baa5-0473a0af770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d958264-e283-47ad-9769-021070804d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceead9b8-bd75-41df-9269-ee73c45ae4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/MotionCLIP\n"
     ]
    }
   ],
   "source": [
    "cd ../MotionCLIP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1bd87-11a7-4c47-87b5-0de6256fdef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03d0372-68e1-49f5-af43-931e0cf88654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.parser.visualize import parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520f83cf-5bd3-4d1b-b011-219addb63637",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters= parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb484653-0c76-41fd-83c4-0ca178708a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'gelu',\n",
       " 'align_pose_frontview': False,\n",
       " 'archiname': 'transformer',\n",
       " 'batch_size': 20,\n",
       " 'clip_image_losses': ['cosine'],\n",
       " 'clip_lambda_ce': 1.0,\n",
       " 'clip_lambda_cosine': 1.0,\n",
       " 'clip_lambda_mse': 1.0,\n",
       " 'clip_lambdas': {'image': {'cosine': 1.0}, 'text': {'cosine': 1.0}},\n",
       " 'clip_map_images': False,\n",
       " 'clip_map_text': False,\n",
       " 'clip_mappers_type': 'no_mapper',\n",
       " 'clip_text_losses': ['cosine'],\n",
       " 'cuda': True,\n",
       " 'datapath': './data/amass_db/amass_30fps_db.pt',\n",
       " 'dataset': 'amass',\n",
       " 'debug': False,\n",
       " 'device': device(type='cuda', index=0),\n",
       " 'expname': 'exps',\n",
       " 'folder': './exps/paper-model',\n",
       " 'glob': True,\n",
       " 'glob_rot': [3.141592653589793, 0, 0],\n",
       " 'jointstype': 'smpl',\n",
       " 'lambda_kl': 0.0,\n",
       " 'lambda_rc': 100.0,\n",
       " 'lambda_rcxyz': 100.0,\n",
       " 'lambda_vel': 100.0,\n",
       " 'lambda_velxyz': 1.0,\n",
       " 'lambdas': {'rc': 100.0, 'rcxyz': 100.0, 'vel': 100.0},\n",
       " 'latent_dim': 512,\n",
       " 'losses': ['rc', 'rcxyz', 'vel'],\n",
       " 'lr': 0.0001,\n",
       " 'max_len': -1,\n",
       " 'min_len': -1,\n",
       " 'modelname': 'cvae_transformer_rc_rcxyz_vel',\n",
       " 'modeltype': 'cvae',\n",
       " 'normalize_encoder_output': False,\n",
       " 'num_epochs': 5000,\n",
       " 'num_frames': 60,\n",
       " 'num_layers': 8,\n",
       " 'num_seq_max': -1,\n",
       " 'pose_rep': 'rot6d',\n",
       " 'sampling': 'conseq',\n",
       " 'sampling_step': 1,\n",
       " 'snapshot': 10,\n",
       " 'translation': True,\n",
       " 'vertstrans': False,\n",
       " 'num_actions_to_sample': 5,\n",
       " 'num_samples_per_action': 5,\n",
       " 'fps': 20,\n",
       " 'appearance_mode': 'motionclip',\n",
       " 'force_visu_joints': True,\n",
       " 'noise_same_action': 'random',\n",
       " 'noise_diff_action': 'random',\n",
       " 'duration_mode': 'mean',\n",
       " 'reconstruction_mode': 'ntf',\n",
       " 'decoder_test': 'new',\n",
       " 'fact_latent': 1,\n",
       " 'images_dir': './action_images',\n",
       " 'zero_global_orient': False,\n",
       " 'ae_after_generation': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d624ed2-3a17-4ca5-a83a-46c4d1a71b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32d30e34-1127-4039-961f-5ca24af1503a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "067d1cca-f8de-41bb-a557-128640f1abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smplx\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed0f9a66-8330-4000-beee-bea8d26c1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/motion_vqvae/body_models/smplh/neutral/model.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e7393-c1d4-442d-8c61-c1a3d08511d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "723fc194-ed9b-436b-b62a-0f1df1dc81c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J_regressor_prior',\n",
       " 'f',\n",
       " 'J_regressor',\n",
       " 'kintree_table',\n",
       " 'J',\n",
       " 'weights_prior',\n",
       " 'weights',\n",
       " 'posedirs',\n",
       " 'bs_style',\n",
       " 'v_template',\n",
       " 'shapedirs',\n",
       " 'bs_type']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npz.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3c6eeb-32fd-486a-8e42-de3c7eee7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for fn in npz.files:\n",
    "    data[fn] = npz[fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30286e23-119f-4a80-9385-77cb41bef0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1873241/1164358967.py:3: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  sm = pickle.load(f, encoding='latin1')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(smpl_p, 'rb') as f:\n",
    "    sm = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cada349f-21f0-4e5d-a7ca-0f861113481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/srv/hays-lab/scratch/sanisetty3/music_motion/motion_vqvae/body_models/smplh/\" + \"SMPLH_NEUTRAL.pkl\", 'wb') as handle:\n",
    "#     sm = pickle.dump(data, handle, encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11b07f7b-2804-4772-a729-9e294f01d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smplh = smplx.SMPLH(path + \"SMPLH_NEUTRAL.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "167101d4-ba2c-4870-b658-1018c9be375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/body_models/smpl/smpl_neutral.npz\" ,sm, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38092a58-785a-49b2-b156-5465444fe06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15623fd5-21d4-4a18-9c7b-2b72e94ec10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "model, preprocess = clip.load('ViT-B/32', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "160f8f47-48c3-415d-a665-e6e76de78042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 72)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "008803da-23b2-4688-9c19-8c75cb6d85ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 18, 3)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(db[\"joints3d\"])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "fd88208a-9e75-455a-ae13-b143bbb43e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([219, 144])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "7402f5f8-be48-4173-80fe-cdfc0019b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "db_file = \"/srv/hays-lab/scratch/sanisetty3/music_motion/MotionCLIP/data/amass_db/amass_30fps_test.pt\"\n",
    "db = joblib.load(db_file)\n",
    "db[\"thetas\"][0].shape\n",
    "\n",
    "ret = geometry.matrix_to_rotation_6d(geometry.axis_angle_to_matrix(torch.Tensor(db[\"thetas\"][0]).reshape(-1,24,3))).reshape(-1,144)\n",
    "\n",
    "joints3D =(db[\"joints3d\"])[0]\n",
    "joints3D = joints3D - joints3D[0, 0, :]\n",
    "ret_tr = torch.Tensor(joints3D)[:, 0, :]\n",
    "              \n",
    "ret_tr = torch.Tensor(ret_tr - ret_tr[0])\n",
    "padded_tr = torch.zeros((ret.shape[0], 6), dtype=ret.dtype)\n",
    "padded_tr[:, :3] = ret_tr\n",
    "ret2 = torch.cat((ret, padded_tr), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe1cf6-d30e-4823-b7a9-e816124acf70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "8edcee87-1abf-4b05-9dd4-58ba13d47e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "92ba285e-4a6a-43aa-a079-a446570f7fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([219, 150])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "69cf6bba-d0cd-4a6b-8d81-9e5a95d06a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "from utils import rotation_conversions as geometry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8817b7c7-e584-481b-8a06-f01fd278434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/srv/hays-lab/scratch/sanisetty3/music_motion/motion_vqvae/body_models/smplh/neutral/model.npz\"\n",
    "smpl_p = \"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/body_models/smpl/smpl_neutral.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22bf47c8-9fd4-41af-9549-0c5a44640728",
   "metadata": {},
   "outputs": [],
   "source": [
    "smplh = BodyModel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "9b9ea8a5-7f9d-4366-9122-b46fd6c95d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aist_axis_angle_from_6d(smpl_motion: torch.Tensor) -> torch.Tensor:\n",
    "    trans = smpl_motion[:, :3]\n",
    "    rots = smpl_motion[:, 3:]\n",
    "\n",
    "    aa_rots = geometry.matrix_to_axis_angle(geometry.rotation_6d_to_matrix(rots.reshape(-1, 22, 6))).reshape(rots.shape[0], -1)\n",
    "    smpl_aa = torch.cat([trans, aa_rots], 1)\n",
    "\n",
    "    return smpl_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "b24ae3e8-7d66-4c15-a219-4dc1fff56202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_clip(motion):\n",
    "    trans = motion[:, :3]\n",
    "    trans = torch.cat([trans , torch.zeros(trans.shape[0], 3)] , 1)\n",
    "    rots = motion[:, 3:]\n",
    "    rots = torch.cat([rots , torch.zeros(rots.shape[0], 12)] , 1)\n",
    "    \n",
    "    new_mot = torch.cat([rots , trans] , 1)\n",
    "    new_mot = new_mot.reshape(1, trans.shape[0] , 25, 6).permute(0,2,3,1)\n",
    "    return new_mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "ae243ed3-66f3-48d1-933f-7c13f0008b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "AIST_GENRE = {\"GBR\" : \"Break\",\n",
    "    \"GPO\" : \"Pop\",\n",
    "    \"GLO\" : \"Lock\",\n",
    "    \"GMH\" : \"Middle Hip-hop\",\n",
    "    \"GLH\" : \"LA style Hip-hop\",\n",
    "    \"GHO\" : \"House\",\n",
    "    \"GWA\" : \"Waack\",\n",
    "    \"GKR\" : \"Krump\",\n",
    "    \"GJS\" : \"Street Jazz\",\n",
    "    \"GJB\" : \"Ballet Jazz\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bafa1271-8f9c-48c7-855b-9bac5b30faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs = torch.cat([clip.tokenize(f\"{c} dance style\") for c in AIST_GENRE.values()]).to(device)\n",
    "text_features = model.encode_text(text_inputs)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "23b01355-cd18-4829-9fd5-a86f6222d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = np.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/new_joint_vecs/00_000000.npy\")\n",
    "motion = torch.Tensor(motion)[:,:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "af4e9d97-dfa6-45eb-ac48-65bc8f5cacb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee57d1-c873-4465-a4d9-0116a1b9f170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "952346a6-2875-4864-a977-e747a2c6aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "\n",
      "             Pop: 37.20%\n",
      "  Middle Hip-hop: 28.40%\n",
      "           House: 14.33%\n",
      "            Lock: 7.38%\n",
      "           Krump: 6.23%\n",
      "LA style Hip-hop: 3.05%\n",
      "     Street Jazz: 1.50%\n",
      "           Waack: 0.83%\n",
      "     Ballet Jazz: 0.82%\n",
      "           Break: 0.27%\n"
     ]
    }
   ],
   "source": [
    "emb = enc.encode_motions(process_for_clip(motion)[...,:60]).to(device)\n",
    "emb /= emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_inputs = torch.cat([clip.tokenize(f\"{c} dance style\") for c in AIST_GENRE.values()]).to(device)\n",
    "text_features = model.encode_text(text_inputs)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "similarity = (100.0 * emb @ text_features.float().T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(10)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{list(AIST_GENRE.values())[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "00431aef-2602-46fa-92a7-761b4d78984d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ced703a8-bba9-42f1-af0f-f49f6f8e866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import codecs as cs\n",
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "fb6d7288-0ca5-45d8-aafc-b8968c7e92af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_smpl_by(input_smpl_6d, y_rot):\n",
    "    if isinstance(y_rot, list):\n",
    "        angl = y_rot\n",
    "    else:\n",
    "        angl = [0,y_rot,0]\n",
    "        \n",
    "    input_smpl = torch.Tensor(input_smpl_6d)\n",
    "    r = R.from_euler('XYZ', angl, degrees=True)\n",
    "    r_matrix = torch.Tensor(r.as_matrix())\n",
    "    r_quat = torch.Tensor(r.as_quat()[[3,0,1,2]])\n",
    "    \n",
    "    \n",
    "    trans = input_smpl[:,:3]\n",
    "    \n",
    "    root_orient = input_smpl[:,3:9]\n",
    "    root_orient_quat = geometry.matrix_to_quaternion(geometry.rotation_6d_to_matrix(root_orient)).reshape(-1,4)\n",
    "    rotated_all_root_quat = geometry.quaternion_multiply(r_quat , root_orient_quat)\n",
    "    \n",
    "    \n",
    "    input_smpl[:,3:9] = geometry.matrix_to_rotation_6d(geometry.quaternion_to_matrix(rotated_all_root_quat))\n",
    "    \n",
    "    input_smpl[:,:3] = trans@r_matrix.T\n",
    "\n",
    "    \n",
    "    return input_smpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f83e34ad-978a-4354-83e6-056daba778e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a184343-1608-40d0-ac5d-1ac90df0bf80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ec1c62b7-485c-4f40-9b9d-09b071754baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "2e8e976b-a01e-4822-b1d1-c949aa63b909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([116, 135])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae14df7-fddb-4014-9255-543d88308bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "732775f8-34b8-4b53-9210-b7cbb6b59ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = glob(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/texts/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43afa486-313f-43f3-a4e4-eaa8ffbc1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption(path):\n",
    "    text_data = []\n",
    "    captions = []\n",
    "    flag = False\n",
    "    with cs.open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            text_dict = {}\n",
    "            line_split = line.strip().split('#')\n",
    "            caption = line_split[0]\n",
    "            captions.append(caption)\n",
    "            tokens = line_split[1].split(' ')\n",
    "            f_tag = float(line_split[2])\n",
    "            to_tag = float(line_split[3])\n",
    "            f_tag = 0.0 if np.isnan(f_tag) else f_tag\n",
    "            to_tag = 0.0 if np.isnan(to_tag) else to_tag\n",
    "    \n",
    "            text_dict['caption'] = caption\n",
    "            text_dict['tokens'] = tokens\n",
    "            if f_tag == 0.0 and to_tag == 0.0:\n",
    "                flag = True\n",
    "                text_data.append(text_dict)\n",
    "            else:\n",
    "                try:\n",
    "                    n_motion = motion[int(f_tag*fps) : int(to_tag*fps)]\n",
    "                    if (len(n_motion)) < min_motion_len or (len(n_motion) >= 200):\n",
    "                        continue\n",
    "                    new_name = random.choice('ABCDEFGHIJKLMNOPQRSTUVW') + '_' + name\n",
    "                    while new_name in data_dict:\n",
    "                        new_name = random.choice('ABCDEFGHIJKLMNOPQRSTUVW') + '_' + name\n",
    "                    data_dict[new_name] = {'motion': n_motion,\n",
    "                                           'length': len(n_motion),\n",
    "                                           'text':[text_dict]}\n",
    "                    new_name_list.append(new_name)\n",
    "                    length_list.append(len(n_motion))\n",
    "                except:\n",
    "                    print(line_split)\n",
    "                    print(line_split[2], line_split[3], f_tag, to_tag, name)\n",
    "                    # break\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb3856-8933-42b6-b850-b230e955711a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f0e42-e239-4c7a-82cd-b5485039f3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94480774-bd18-4b50-9cad-216b99da62a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = get_caption( \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/texts/000000.txt\")\n",
    "captions_opp = get_caption(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/texts/001000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "349bdd76-6457-4572-8e27-c34024b9923a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a man kicks something or someone with his left leg.',\n",
       " 'the standing person kicks with their left foot before going back to their original stance.',\n",
       " 'a man kicks with something or someone with his left leg.',\n",
       " 'he is flying kick with his left leg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "a89bdf39-2c31-44fc-ac82-8f97f9caa105",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_caps = captions + captions_opp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "c18b0048-2e0f-4860-a19f-3737398a3075",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = np.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/new_joint_vecs/00_000000.npy\")\n",
    "motion = torch.Tensor(motion)[:,:135]\n",
    "nm = rotate_smpl_by(motion, [90,-90,0])\n",
    "\n",
    "motion1 = np.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/new_joint_vecs/180_000000.npy\")\n",
    "motion1 = torch.Tensor(motion1)[:,:135]\n",
    "nm1 = rotate_smpl_by(motion1, [90,-90,0])\n",
    "\n",
    "motion2 = np.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/new_joint_vecs/00_001000.npy\")\n",
    "motion2 = torch.Tensor(motion2)[:,:135]\n",
    "nm2 = rotate_smpl_by(motion2, [90,-90,0])\n",
    "\n",
    "motion3 = np.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/000000_og.npy\")\n",
    "motion3 = torch.Tensor(motion3)[:,:135]\n",
    "nm3 = rotate_smpl_by(motion3, [90,-90,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d5834c-326d-491c-b784-96a8c9d3d9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89114ca0-00b4-4343-83f7-5a2a8c2c1c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0dbbe-b1ac-47b1-9e62-ee81109c8e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "ce0aa036-a916-40e5-8bfc-e7de2ab7aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = torch.cat([process_for_clip(motion)[...,:60] , process_for_clip(motion1)[...,:60], process_for_clip(motion2)[...,:60], process_for_clip(motion3)[...,:60]] , 0)\n",
    "emb = enc.encode_motions((mms)).to(device)\n",
    "text_inputs = torch.cat([clip.tokenize(c) for c in all_caps]).to(device)\n",
    "text_features = model.encode_text(text_inputs).float()\n",
    "\n",
    "emb /= emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "seq_motion_features_norm = emb / emb.norm(dim=-1, keepdim=True)\n",
    "logit_scale = model.logit_scale.exp()\n",
    "logits_per_motion = logit_scale * seq_motion_features_norm @ features_norm.t()\n",
    "logits_per_d = logits_per_motion.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "5f079932-efd1-47c6-ae47-e61d690cc8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb /= emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22a3ef-89a2-49dc-af17-700a97830771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "044a219a-95d2-4056-9b88-258698959392",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "seq_motion_features_norm = emb / emb.norm(dim=-1, keepdim=True)\n",
    "logit_scale = model.logit_scale.exp()\n",
    "logits_per_motion = logit_scale * seq_motion_features_norm @ features_norm.t()\n",
    "logits_per_d = logits_per_motion.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "ade77761-593c-4562-a5bf-cb883877f315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[68.5927, 66.7999, 68.5085, 66.7495, 73.3126, 70.6648, 70.4461],\n",
       "        [68.4425, 66.9240, 68.2853, 66.9656, 73.3869, 70.6454, 70.3920],\n",
       "        [67.5542, 67.4931, 67.5023, 65.7932, 73.7883, 70.8619, 71.4676],\n",
       "        [67.7534, 66.2508, 67.4441, 65.6629, 72.3491, 69.3443, 69.6399]],\n",
       "       device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_per_motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b2bbca5d-5f00-457a-a1bb-67733c08da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = logits_per_motion.softmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "4933e8ba-63f5-4a95-b98f-3a0b93275fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "\n",
      "a person does a long step to the right.: 87.13%\n",
      "someone taking a forward-right step in , leading with their right foot.: 6.17%\n",
      "person takes a step diagonally forward and to the right with their right foot followed by their left.: 4.96%\n",
      "a man kicks something or someone with his left leg.: 0.78%\n",
      "a man kicks with something or someone with his left leg.: 0.71%\n",
      "the standing person kicks with their left foot before going back to their original stance.: 0.13%\n",
      "he is flying kick with his left leg: 0.12%\n"
     ]
    }
   ],
   "source": [
    "values, indices = similarity[0].topk(len(all_caps))\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{all_caps[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "fbd96559-fef2-4527-9221-255f7e3f46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "\n",
      "a person does a long step to the right.: 86.50%\n",
      "someone taking a forward-right step in , leading with their right foot.: 5.95%\n",
      "person takes a step diagonally forward and to the right with their right foot followed by their left.: 5.44%\n",
      "a man kicks something or someone with his left leg.: 0.97%\n",
      "a man kicks with something or someone with his left leg.: 0.80%\n",
      "he is flying kick with his left leg: 0.19%\n",
      "the standing person kicks with their left foot before going back to their original stance.: 0.15%\n"
     ]
    }
   ],
   "source": [
    "values, indices = similarity[1].topk(len(all_caps))\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{all_caps[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d7bd1c98-0c3b-4c49-b303-f5d02f28cbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "\n",
      "a person does a long step to the right.: 88.85%\n",
      "person takes a step diagonally forward and to the right with their right foot followed by their left.: 4.84%\n",
      "someone taking a forward-right step in , leading with their right foot.: 4.72%\n",
      "a man kicks something or someone with his left leg.: 0.81%\n",
      "a man kicks with something or someone with his left leg.: 0.53%\n",
      "the standing person kicks with their left foot before going back to their original stance.: 0.13%\n",
      "he is flying kick with his left leg: 0.11%\n"
     ]
    }
   ],
   "source": [
    "values, indices = similarity[3].topk(len(all_caps))\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{all_caps[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24c92a-0577-4d96-9f02-774c58a702e5",
   "metadata": {},
   "source": [
    "## Do motionclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "3bbd07dc-8c21-41b8-b881-52788d687afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.models.motion_clip.motion_clip import MotionClipEncoder\n",
    "\n",
    "ckpt = torch.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/core/models/motion_clip/MotionClipEncoder.pt\", map_location=\"cpu\")\n",
    "enc = MotionClipEncoder()\n",
    "enc.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "8d093285-a587-41bb-ae80-e68a79fd653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "model, preprocess = clip.load('ViT-B/32', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "fb881d7b-fdfb-4a83-8490-f145edc2dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_clip(motion):\n",
    "\n",
    "    if motion.shape[-1] == 135:\n",
    "        trans = motion[:, :3]\n",
    "        trans = torch.cat([trans , torch.zeros(trans.shape[0], 3)] , 1)\n",
    "        rots = motion[:, 3:]\n",
    "        rots = torch.cat([rots , torch.zeros(rots.shape[0], 12)] , 1)\n",
    "        \n",
    "        new_mot = torch.cat([rots , trans] , 1)\n",
    "    new_mot = new_mot.reshape(1, motion.shape[0] , 25, 6).permute(0,2,3,1)\n",
    "    return new_mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "df10c5fb-bcbd-474e-a308-efddf93e333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "db_file = \"/srv/hays-lab/scratch/sanisetty3/music_motion/MotionCLIP/data/amass_db/amass_30fps_test.pt\"\n",
    "db = joblib.load(db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "820d33e2-6a5c-46e0-8238-8e321010ee3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vid_names', 'thetas', 'joints3d', 'clip_images', 'clip_pathes', 'text_raw_labels', 'text_proc_labels', 'action_cat'])"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "e176e509-2b49-4c82-891d-25ced215613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMotion(idx):\n",
    "    ret = geometry.matrix_to_rotation_6d(geometry.axis_angle_to_matrix(torch.Tensor(db[\"thetas\"][idx]).reshape(-1,24,3))).reshape(-1,144)\n",
    "    \n",
    "    joints3D =(db[\"joints3d\"])[idx]\n",
    "    joints3D = joints3D - joints3D[0, 0, :]\n",
    "    ret_tr = torch.Tensor(joints3D)[:, 0, :]\n",
    "                  \n",
    "    ret_tr = torch.Tensor(ret_tr - ret_tr[0])\n",
    "    padded_tr = torch.zeros((ret.shape[0], 6), dtype=ret.dtype)\n",
    "    padded_tr[:, :3] = ret_tr\n",
    "    ret2 = torch.cat((ret, padded_tr), 1)\n",
    "\n",
    "    return ret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "59c4402c-853d-4c95-bdf5-7f361983bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_og = getMotion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d831b-38ea-44fd-b99c-354e06163b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a368d4-c444-42ca-a2ce-b2edcb636092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "dcdeb15b-499a-4fcf-acf3-df7b80a26cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "motionn = np.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/new_joint_vecs/00_001612.npy\")\n",
    "motionn = torch.Tensor(motionn)[:,:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a45b73f3-62ca-483a-a277-a2577ebe93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = get_caption( \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/texts/001612.txt\")\n",
    "captions_opp = get_caption(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/texts/001000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "b3fe8d1a-e640-454a-8219-ba0ea414819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_caps = captions + captions_opp + [\"jumping jack\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "391a027d-d1b8-4fd6-ac76-cd0ac2b478e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "motions = [motionn , motion_og]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "6f74c0a8-c7c0-4f73-8b69-cdebb49a87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = torch.cat([process_for_clip(motionn)[...,:60] for motion in motions] , 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "b6b0ee70-6275-4d4e-aa67-65631ee0e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = enc.encode_motions(mms).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "0dc81dc5-5d84-4235-bace-08f535ee2d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb /= emb.norm(dim=-1, keepdim=True)\n",
    "text_inputs = torch.cat([clip.tokenize(c) for c in all_caps]).to(device)\n",
    "text_features = model.encode_text(text_inputs).float()\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "similarity = (100.0 * emb @ text_features.float().T).softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "5d24e819-ef96-489e-bc55-5e14d22b5667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0071, 0.0129, 0.1549, 0.6157, 0.0876, 0.0816, 0.0402],\n",
       "        [0.0079, 0.0145, 0.1718, 0.5888, 0.0904, 0.0733, 0.0533]],\n",
       "       device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "cb5d4dd8-28ef-4213-a5aa-9f6eaf29f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "\n",
      "a person does a long step to the right.: 61.57%\n",
      "a person does jumping jacks followed by a twist.: 15.49%\n",
      "someone taking a forward-right step in , leading with their right foot.: 8.76%\n",
      "person takes a step diagonally forward and to the right with their right foot followed by their left.: 8.16%\n",
      "    jumping jack: 4.02%\n",
      "person does jumping jacks and does a turn around hop at the end.: 1.29%\n",
      "doing exercizes with jumping jacks: 0.71%\n"
     ]
    }
   ],
   "source": [
    "values, indices = similarity[0].topk(len(all_caps))\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{all_caps[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "b58e5670-60b8-4c73-b8d8-4bdd938421b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "\n",
      "a person does a long step to the right.: 58.88%\n",
      "a person does jumping jacks followed by a twist.: 17.18%\n",
      "someone taking a forward-right step in , leading with their right foot.: 9.04%\n",
      "person takes a step diagonally forward and to the right with their right foot followed by their left.: 7.33%\n",
      "    jumping jack: 5.33%\n",
      "person does jumping jacks and does a turn around hop at the end.: 1.45%\n",
      "doing exercizes with jumping jacks: 0.79%\n"
     ]
    }
   ],
   "source": [
    "values, indices = similarity[1].topk(len(all_caps))\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{all_caps[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24b36e-499e-4327-b2c7-35d3ba020f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90715f-eb5f-4cf3-943d-2b0d7718077c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
