{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c210f3-99ba-40e8-bf38-3373f13bca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1217d9bc-fc3e-492a-b216-2ee9fb7086e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b096c1d9-386e-4b0e-ab6f-f2daf9b1a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator\n",
    "from einops import rearrange, reduce, pack, unpack\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "debdf11b-1fc3-4e7e-b273-f68ca059a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e057cc3-7400-46b9-8dc9-9e5c45b1a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.conv_vqvae import ConvVQMotionModel\n",
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.conformer_vqvae import ConformerVQMotionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e5ba5c-201b-4b70-8443-25244ae22258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from: /srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_512_1024_affine/conformer_512_1024_affine.yaml\n"
     ]
    }
   ],
   "source": [
    "path = \"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_512_1024_affine/conformer_512_1024_affine.yaml\"\n",
    "cfg = get_cfg_defaults()\n",
    "print(\"loading config from:\", path)\n",
    "cfg.merge_from_file(path)\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2076150b-628e-47a3-be4c-6883b3631f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync is turned on False\n"
     ]
    }
   ],
   "source": [
    "vqvae_model = ConformerVQMotionModel(\n",
    "    cfg.vqvae,\n",
    "    # is_distributed=self.is_distributed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03c02e84-e74d-4584-9b6e-2fdb79450650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training params: 16.58M\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in vqvae_model.motionEncoder.parameters() if p.requires_grad)\n",
    "print(\"Total training params: %.2fM\" % (total / 1e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc816d7-e1be-4691-b2a4-ea161c98a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1,80,271))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a842909-8cc4-493b-82df-ed88e80ec12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, ind, lss = vqvae_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de82ac13-7ed7-49c8-a7d7-808428b598c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 271])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab8b076b-e029-41a4-aac3-283bd8c43749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.vqvae.depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c15abb-1163-4b93-b2c3-8249fb18f8f1",
   "metadata": {},
   "source": [
    "## Calc mean comman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9f407c-703f-4cbc-8ea2-49126fc8b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "hml = \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D_SMPL\"\n",
    "aist = \"/srv/hays-lab/scratch/sanisetty3/music_motion/AIST_SMPL\"\n",
    "cm =\"/srv/hays-lab/scratch/sanisetty3/music_motion/Choreomaster_SMPL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e23a0d-7dab-4eeb-88d6-8c71cee1a9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76ebff3a-6191-4726-88e7-026e7896bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = np.load(os.path.join(hml, \"Mean.npy\"))\n",
    "ma = np.load(os.path.join(aist, \"Mean.npy\"))\n",
    "mc = np.load(os.path.join(cm, \"Mean.npy\"))\n",
    "\n",
    "sh = np.load(os.path.join(hml, \"Std.npy\"))\n",
    "sa = np.load(os.path.join(aist, \"Std.npy\"))\n",
    "sc = np.load(os.path.join(cm, \"Std.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adb0af74-8977-4f6b-8da7-332720224716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b310fc4d-1af5-49b6-89ee-364f1e576568",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.mean([mh, ma,mc] , 0)\n",
    "s = np.mean([sh,sa,sc] , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fab7e4d4-0a19-43a4-aa7f-9e83cd2dfe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/pretrained/common/Std.npy\" , s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27211f97-c964-438a-8087-0c99f740ea04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ab72500-b7a3-4907-b9d9-acff965778e1",
   "metadata": {},
   "source": [
    "## Muse stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "feaf7d2b-0414-4181-a99e-dfebccbee802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32767b3e-6018-43e0-9a04-f6591166b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AttentionParams:\n",
    "    dim: int = 768\n",
    "    dim_head: int = 96\n",
    "    heads: int = 8\n",
    "    causal: bool = False\n",
    "    qk_norm: bool = False\n",
    "    qk_norm_scale: int = 8\n",
    "    dropout: float = 0.0\n",
    "    cross_attn_tokens_dropout: float = (0.0,)\n",
    "    add_null_kv: bool = False\n",
    "    flash: bool = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08318807-b406-41de-8da0-49bd0f0ad154",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AttentionParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfbd23a0-b464-4a69-8a50-128d6c307af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.causal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9531a2c0-a035-465d-8317-263987888a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class PositionalEmbeddingType(Enum):\n",
    "    REL= \"RelativePositionBias\"\n",
    "    SINE= \"ScaledSinusoidalEmbedding\"\n",
    "    ALIBI= \"AlibiPositionalBias\"\n",
    "    ABS= \"AbsolutePositionalEmbedding\"\n",
    "    SHAW= \"ShawRelativePositionalEmbedding\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "167c7a17-8992-4352-b4dd-0055c70a4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PositionalEmbeddingType.REL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f71415fa-a1a7-4871-bfc7-38a91b882f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((4,100,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c5152-67cd-4108-b259-351d88d44538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d481b5ca-759d-47f3-b027-2e154c51618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = nn.InstanceNorm1d(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c95a016-c267-4d19-bd25-d12924754f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(shape, min=0, max=1, device=None):\n",
    "    return torch.zeros(shape, device=device).float().uniform_(0, 1)\n",
    "\n",
    "\n",
    "def prob_mask_like(shape, prob, device=None):\n",
    "    if prob == 1:\n",
    "        return torch.ones(shape, device=device, dtype=torch.bool)\n",
    "    elif prob == 0:\n",
    "        return torch.zeros(shape, device=device, dtype=torch.bool)\n",
    "    else:\n",
    "        return uniform(shape, device=device) < prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4cc56ccf-ab2b-466f-9e09-d5683d911036",
   "metadata": {},
   "outputs": [],
   "source": [
    "b,n,d = a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61884ec7-39ff-462c-9274-b04105ffbe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ = prob_mask_like((b, 1), 1.0 - 0.5, \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5059994-9e58-4b05-9321-0bb41d86c74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c6c4acb-4280-4f65-8170-a37b08e25c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_mask = (a != 1026).any(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832189d-a1f1-4efe-a527-0d5a3f36176a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1522c7f-ecf5-4e7c-b36c-b3f6aae0654f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72722d35-207c-4118-9ccd-5321d33ddbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_mask = context_mask & mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86326fd-ebaf-4c10-9e47-fe84919184c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d0372-68e1-49f5-af43-931e0cf88654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
