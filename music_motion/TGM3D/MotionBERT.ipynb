{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabfd305-59ca-4d5f-b319-b7151f135a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346704e-c584-4431-8c28-5282e76c8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f82299-7ba4-4229-b3da-7b1ee47f358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9110ff05-8603-4916-83a3-909926251190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import get_cfg_defaults\n",
    "from core.datasets.dataset_loading_utils import load_dataset\n",
    "from core.datasets.vq_dataset import DATALoader\n",
    "from utils.vis_utils import plot_3d_global\n",
    "from core.models.conformer_vqvae import ConformerVQMotionModel, Encoder\n",
    "from torch.utils import data\n",
    "from core.datasets.vq_dataset import DATALoader, MotionCollator\n",
    "from einops import pack, rearrange, reduce, repeat, unpack\n",
    "\n",
    "def pack_one(t, pattern):\n",
    "    return pack([t], pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7569282a-d096-439a-afc8-1a3184229695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.vis_utils.plot_3d_global as plot_3d\n",
    "from utils.motion_processing.hml_process import recover_from_ric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887cb036-5619-469a-b735-8d4e495c97f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a3688bb-c984-43bf-ade8-f55d8047318c",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e790c066-3169-472f-b9e2-9d4773b86569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vis_utils.render_final import Renderer\n",
    "renderer = Renderer(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ba701-a4a9-4e72-8158-4fee24ab019c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96f701-5a8d-433d-b688-5b9302cf4eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "782bde2e-f8b9-4d44-8a92-01c6913ee68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from: /srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_768_1024_hmlvec/conformer_768_1024_hmlvec.yaml\n",
      "tensor([200000.])\n",
      "Sync is turned on False\n"
     ]
    }
   ],
   "source": [
    "path = \"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_768_1024_hmlvec/conformer_768_1024_hmlvec.yaml\"\n",
    "cfg = get_cfg_defaults()\n",
    "print(\"loading config from:\", path)\n",
    "cfg.merge_from_file(path)\n",
    "cfg.freeze()\n",
    "\n",
    "ckpt = torch.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_768_1024_hmlvec/vqvae_motion.pt\" , map_location=\"cpu\")\n",
    "print(ckpt[\"steps\"])\n",
    "\n",
    "from core.models.conformer_vqvae import ConformerVQMotionModel, Encoder\n",
    "convvq = ConformerVQMotionModel(cfg.vqvae).to(device).eval()\n",
    "convvq.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_768_1024_hmlvec/vqvae_motion.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbc534c7-3e84-47da-b67a-d3ee3d129cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ../motion_vqvae/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a942e0-2637-425d-8048-0242c26472a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from configs.config import get_cfg_defaults\n",
    "\n",
    "# path = \"/srv/hays-lab/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/conv_vq/convq_512_512/convq_512_512.yaml\"\n",
    "# cfg = get_cfg_defaults()\n",
    "# print(\"loading config from:\", path)\n",
    "# cfg.merge_from_file(path)\n",
    "# cfg.freeze()\n",
    "\n",
    "# ckpt = torch.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/conv_vq/convq_512_512/vqvae_motion.pt\" , map_location=\"cpu\")\n",
    "# print(ckpt[\"steps\"])\n",
    "\n",
    "# from motion_vqvae.core.models.conv_vqvae import ConvVQMotionModel\n",
    "# convvq = ConvVQMotionModel(cfg.vqvae).to(device).eval()\n",
    "\n",
    "# convvq.load_state_dict(ckpt[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbd7d261-56a2-4eee-a26e-349d7cd5ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "alll = glob(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/AIST/new_joint_vecs/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540078a7-9ac3-4036-a198-5ad87730f5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2279191-e562-4415-b457-32010c41368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/AIST_SMPL/all.txt\" , \"w\") as f:\n",
    "    for line in alll:\n",
    "        f.write(f'{line.split(\"/\")[-1].split(\".\")[0]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5db0c61-4447-4fd1-b7df-708828aaa2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vq_dataset import VQMotionDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a0544fc-47d1-41a9-a2f1-39853636e69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:00<00:00, 496.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQMotionDataset(\"cm\" , \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion\" , window_size = -1, split = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d04642-c1b3-4dd1-81e9-85a501297d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f90ff4ef-c578-4059-b32d-f9b92541444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DATALoader(\n",
    "            train_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            collate_fn=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b1f28f4-85d3-47d5-8f21-52db9c34b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dl:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ee5717e-4508-4a34-901a-f8e8672664b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest = \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionIndices/HumanML3D/joint_indices\"\n",
    "os.makedirs(dest, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5a758-e991-43b2-a9bc-3793c7beaa84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e379da16-09be-4606-abb0-2ac0cb3d8efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2726/2726 [02:14<00:00, 20.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(train_dl)):\n",
    "    # if i < 12300:\n",
    "    #     continue\n",
    "    \n",
    "    gt_motion = batch[\"motion\"].to(device)\n",
    "    if gt_motion.shape[1] > 100:\n",
    "        ind = []\n",
    "        for m in range(0, gt_motion.shape[1], 100):\n",
    "            indics = convvq.encode(gt_motion[:, m:m+100])\n",
    "            ind.append(indics[0])\n",
    "        indices = torch.cat(ind)[None]\n",
    "    else:\n",
    "        indices = convvq.encode(gt_motion)\n",
    "    np.save(os.path.join(dest , batch[\"names\"][0]+\".npy\") , indices.detach().cpu().numpy())\n",
    "    del indices\n",
    "    del gt_motion\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d08a12-9145-4b6f-b694-70508fafec17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a27f53-1229-434f-961a-07d0aaf4115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPLIndices/AIST/joint_indices/M_gJS_sBM_cAll_d03_mJS3_ch02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6c717-21f4-4650-a55d-dbe7e97ecf52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91969d6-e88d-47f2-aae2-ae55f42e306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "og = f\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/HumanML3D/new_joint_vecs/{batch['names'][0]}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d666be5-1052-4fe7-be3a-fb86d1fd85b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3205, 263])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dl:\n",
    "    break\n",
    "gt_motion = batch[\"motion\"][:,:1000]\n",
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "700c4514-feac-4201-bf31-c11c3940cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gt_motion.shape[1] > 200:\n",
    "    ind = []\n",
    "    for m in range(0, gt_motion.shape[1], 80):\n",
    "        indics = convvq.encode(gt_motion[:, m:m+80].to(device))\n",
    "        ind.append(indics[0])\n",
    "    indices = torch.cat(ind)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac98f604-e528-49a2-a224-bbc3c09bb716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 250])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a30a9c6-cb17-4030-8c82-c0c0d66316d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 263])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized, decoded_motion_features = convvq.decode(indices.long())\n",
    "decoded_motion_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f54e14dd-554c-4bfc-a4b9-bec8496d939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion1 = (\n",
    "    train_ds.inv_transform(gt_motion.cpu())\n",
    "    .squeeze()\n",
    "    .float()\n",
    ")\n",
    "pred_motion = (\n",
    "    train_ds.inv_transform(decoded_motion_features.cpu())\n",
    "    .squeeze()\n",
    "    .float()\n",
    ")\n",
    "\n",
    "save_file = \"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/renders\"\n",
    "gt_motion_xyz = recover_from_ric(gt_motion1, 22)\n",
    "pred_motion_xyz = recover_from_ric(pred_motion, 22)\n",
    "\n",
    "gt_pose_vis = plot_3d.draw_to_batch(\n",
    "    gt_motion_xyz.numpy().squeeze()[None],\n",
    "    None,\n",
    "    [os.path.join(save_file, \"t\" + \"_gt.gif\")],\n",
    ")\n",
    "pred_pose_vis = plot_3d.draw_to_batch(\n",
    "    pred_motion_xyz.numpy().squeeze()[None],\n",
    "    None,\n",
    "    [os.path.join(save_file, \"t\" + \"_pred.gif\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3320e-1e06-45fe-87b8-044d9d74e4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56c066d3-cd58-47ae-afa0-66e71517022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from core.datasets import dataset_TM_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b4512-71c3-45b7-bc24-ad36aece8c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93364710-fcda-4a8b-9ad5-631c8dcddd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4384/4384 [00:04<00:00, 1066.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4248 4248\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w_vectorizer = WordVectorizer(\n",
    "   \"/srv/hays-lab/scratch/sanisetty3/music_motion/T2M-GPT/glove\", \"our_vab\"\n",
    ")\n",
    "eval_wrapper = EvaluatorModelWrapper(cfg.eval_model)\n",
    "tm_eval = dataset_TM_eval.DATALoader(\n",
    "    32,\n",
    "    w_vectorizer,\n",
    "    unit_length=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a365f4d9-e219-411e-b2fd-385119434eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_trans import calculate_R_precision, calculate_multimodality, calculate_diversity, calculate_frechet_distance, calculate_activation_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ddcd6-1f68-4909-8bb2-707c0533be2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "afe3d6cd-c3b4-4b34-bdfb-3c2810b66def",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer.render(\n",
    "    motion_vec=ogg[:1000,:135],\n",
    "    outdir=\"./renders/\",\n",
    "    step=0,\n",
    "    name=f\"00_000007_og\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8092f2-8f5f-440d-9eb8-bcbe1f9b29a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94880c46-de79-4e1d-9ff0-db7163a10faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb15fcc-f4c4-4fd2-a88e-352526e906f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6bd0c2e-bec7-47bc-b115-5c59078ebfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25bef9be-44eb-44c9-992f-6a2356f5b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hml = \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/HumanML3D/Mean.npy\"\n",
    "hml2 = \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/AIST/Mean.npy\"\n",
    "hml3 = \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/Choreomaster/Mean.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "268210f1-5360-4eb4-b256-069a8bf897c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/Mean.npy\" , np.mean([np.load(hml) + np.load(hml2) + np.load(hml3)] , 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d869b8-f7d6-47dd-889f-af852f8f22bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c71ac0fc-62aa-4b5f-ad6a-7867c1a7d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "db0eccd4-791d-4386-b107-bcff752f6331",
   "metadata": {},
   "outputs": [],
   "source": [
    "pths = sorted(glob(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/HumanML3D/new_joint_vecs/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aecd03dc-f424-4e79-841b-d505cf7c5443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/HumanML3D/new_joint_vecs/M008676.npy'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pths[25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "47310a88-0986-4cde-b532-e7ea13b1db5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8676"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(pths[25000].split(\"/\")[-1].split(\".\")[0][-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8eff3e1e-adb4-42fa-bd00-4cf93073bad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 32648/32648 [00:00<00:00, 964613.99it/s]\n"
     ]
    }
   ],
   "source": [
    "add = []\n",
    "for p in tqdm(pths):\n",
    "    nm = int(p.split(\"/\")[-1].split(\".\")[0][-6:])\n",
    "    if nm > 14616:\n",
    "        add.append(p.split(\"/\")[-1].split(\".\")[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "59791185-879b-4e1e-b45c-85be057e000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open(r'/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotion/HumanML3D/train.txt', 'a') as fp:\n",
    "    for item in add:\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5b9d3c0b-51e0-4b6a-aabb-533b387cee42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3418"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "546060ca-56b9-40dc-93a8-d57fd9a43d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 32648/32648 [00:00<00:00, 1130149.03it/s]\n"
     ]
    }
   ],
   "source": [
    "add = []\n",
    "for p in tqdm(pths):\n",
    "    n = p.split(\"/\")[-1].split(\".\")[0]\n",
    "    add.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2ac9da65-d194-4b66-b00a-84ad44969eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9329c4-7e76-496b-b0a5-2fa740078a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce3ff326-0237-48d3-bf91-0490039d8d65",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65831f99-a68e-49ae-9a85-7958f2de0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.dataset_loading_utils import load_dataset_bert\n",
    "from core.datasets.motion_bert_dataset import BERTMotionDataset, DATALoader, BERTMotionDatasetSimplified\n",
    "from core.models.BERT import BERT, BERTParams\n",
    "from core.optimizer import get_optimizer\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0cb4b2-cc2e-4bf4-b19d-be127f5889a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d65d85e2-6d32-4a2d-860e-ee2aad75baf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from: /srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/bert_12_768/bert_12_768.yaml\n"
     ]
    }
   ],
   "source": [
    "path = \"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/bert_12_768/bert_12_768.yaml\"\n",
    "cfg = get_cfg_defaults()\n",
    "print(\"loading config from:\", path)\n",
    "cfg.merge_from_file(path)\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06bd026a-4b67-4ee0-80d0-492cd2cde41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4384/4384 [00:02<00:00, 1603.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 3516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds, sampler_train, weights_train = load_dataset_bert(\n",
    "                dataset_names=[\"t2m\"],\n",
    "                args=cfg,\n",
    "                split=\"test\",\n",
    "                weight_scale=[1],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6a720e4-9f65-4887-b8cb-3bd45c90deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DATALoader(\n",
    "            train_ds,\n",
    "            batch_size=2,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee9c44c7-1fcf-4a4f-9139-7bfb6b34c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = BERTParams()\n",
    "bert = BERT(params).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f0b639e-f86c-43a6-9ee3-784d56de0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a563d69-f41f-4745-ad7c-e375d84f9f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bert_input', 'bert_label'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "134e8ccb-b5f9-4933-ae98-9b3ea573d91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1024,   -1,  611,  242,   -1,   -1,  193,   -1,   -1,  736,  839,   -1,\n",
       "          -1,   -1,  922,   -1,   -1,   -1,  845,   -1,   -1,   -1,  552,  358,\n",
       "          -1,   -1, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"bert_label\"][0 , :27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b99cbd8-19d1-4d20-a015-26523c091294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25, 25],\n",
       "        [17, 45]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"motion_lengths\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d897039-c34c-4266-ad33-5a7bde995103",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_loss_fnc = torch.nn.NLLLoss(ignore_index=0)\n",
    "mlm_loss_fnc = torch.nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0766820c-ee43-4ce1-91a3-089ddfcd550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_lm_output = bert.forward(data[\"bert_input\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d119bc-63b3-4434-be21-a87dd15e3962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8238d8c7-60ce-4583-8a52-777555f3903e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "769b9d8f-fc73-41fc-9616-f12ebe6b36fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1024,   -1,   -1,   -1,  370,   -1,   -1,   -1,  888,   -1,   -1,  699,\n",
       "          -1,   -1,   -1,   -1,   -1,  699,  845,   -1,   -1,   -1,   -1,   -1,\n",
       "         148,   -1,   -1,   -1,   -1,  197,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "          -1,  699,  447,   -1,  177,  591,   -1,   -1,   -1,  999,   -1,  142,\n",
       "          -1,  627,   -1, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,\n",
       "        1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"bert_label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5083ce89-4df6-4efc-97d9-a17cfb70c3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1027])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_lm_output.transpose(1, 2).reshape(-1,1027).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea44130c-b80f-41cd-a2c1-050e5a535208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_sent_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7247598c-e113-4c6d-bdd7-951ca029f99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"is_next\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4108e18e-1ce7-453a-84a1-368bf1b3b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_loss = nsp_loss_fnc(next_sent_output, data[\"is_next\"].cuda().reshape(-1))\n",
    "\n",
    "# 2-2. NLLLoss of predicting masked token word\n",
    "mask_loss = mlm_loss_fnc(\n",
    "    mask_lm_output.transpose(1, 2), data[\"bert_label\"].cuda()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9d8c729-66f6-4cd8-a415-c7f50a577b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.0331, device='cuda:0', grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d16b9344-eb7a-4a42-b560-ac6680b00350",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = (\n",
    "                    next_sent_output.argmax(dim=-1).eq(data[\"is_next\"].cuda()).sum().item()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d638cccd-b3f0-4b89-aa7c-4f46eced4aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_sent_output.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "363b0b30-38e1-4a3d-8b24-b9b88de3bdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"is_next\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72bea50d-96ab-4d70-ba24-02a205fddcb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_sent_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42af97d5-dd9f-40cb-845e-765ab60081c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "    mask_excess = mask.cumsum(dim=-1) > (num_tokens * prob).ceil()\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    return new_mask[:, 1:].bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a1848-c059-44b6-8d2a-337307d71be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baad915e-1884-475b-bf47-f859abcd1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ignore_token_ids = [1025,1026, 1027]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861125b2-bb3c-441e-a1a5-052aaf65e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c33fe4-8fd5-4dbb-9b52-cd8ed9adf153",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_mask = mask_with_tokens(seq, self.mask_ignore_token_ids)\n",
    "mask = get_mask_subset_with_prob(~no_mask, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7721fd-f87c-4579-bf3e-ae3a72f67dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
