{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8005eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34652c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b835508",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c9379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13870fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator\n",
    "from einops import rearrange, reduce, pack, unpack\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b795d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4d41a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share2/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.datasets.dataset_loading_utils import load_dataset\n",
    "from core.datasets.vq_dataset import DATALoader\n",
    "from utils.vis_utils import plot_3d_global\n",
    "from core.models.conv_vqvae import ConvVQMotionModel\n",
    "from core.models.conformer_vqvae import ConformerVQMotionModel\n",
    "import utils.rotation_conversions as geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb6cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81d9478",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_768_1024_affine_varlen/conformer_768_1024_affine_varlen.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84005eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.dataset.dataset_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6e81871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync is turned on False\n"
     ]
    }
   ],
   "source": [
    "vqvae_model = ConformerVQMotionModel(\n",
    "            cfg.vqvae,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "963292d0-ef04-4ca0-b51a-930dc202dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/checkpoints/conformer_768_1024_affine_varlen/vqvae_motion.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cedbb8d",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "324cba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6221/6221 [01:04<00:00, 95.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 5329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "render_ds, sampler_val, weights_val = load_dataset(\n",
    "                [\"t2m\"],cfg, \"test\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad21dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_dl = DATALoader(\n",
    "    \n",
    "            render_ds, batch_size=32, shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab489b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00_000000']\n"
     ]
    }
   ],
   "source": [
    "batch = (next(iter(render_dl)))\n",
    "    \n",
    "print(batch[\"names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f77fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vis_utils.render_final import Renderer\n",
    "renderer = Renderer(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c846bfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f4b81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bc87b9bb-7ced-42c2-8fb1-bc22d2ccbd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = model.motionEncoder( torch.randn((1, 80 , 271)).cuda() , True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7215d9-ab58-46b1-a4d9-a49302ca26d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a9305d-685b-43c2-bac0-8746bb480a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fcd35571-277b-4c36-888d-b644d4072c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_motion_features = aa.mean() + torch.randn((1, 20 , 768)).to(model.device) * aa.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e254e7aa-1bf3-4b53-8735-362de6834748",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_enc_motion, indices, commit_loss = model.vq(embed_motion_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "91f57a12-f9c0-43fc-9b00-0ece08f8ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_motion_features = model.motionDecoder(quantized_enc_motion, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9271fb-bcab-41a9-98a1-05aa962f380b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015e97b-9011-4914-9ee9-e723ce0dac83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eddf953-35fd-4e85-825f-b73e51156580",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_path = \"/srv/hays-lab/scratch/sanisetty3/music_motion/TGM3D/renders/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6a55068-e766-41db-888c-c7079a184cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = np.load('/coc/scratch/sanisetty3/music_motion/MOYO/new_joints_smpl/220923_yogi_body_hands_03596_Boat_Pose_or_Paripurna_Navasana_-b_stageii.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "996e248c-0a32-4504-8db5-6da4ca003349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261, 135)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e037355-8d5d-442d-ba72-de3882b735a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rots = geometry.matrix_to_rotation_6d(geometry.axis_angle_to_matrix(torch.Tensor(data[:,3:69].reshape(-1,22,3)))).reshape(-1,132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7508c078-44d4-4059-98f8-86a5c57825cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "401726a9-8d61-4539-9920-e7ad9b133cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 135)\n"
     ]
    }
   ],
   "source": [
    "# data  = np.load('/coc/scratch/sanisetty3/music_motion/MOYO/new_joints_smpl/220923_yogi_body_hands_03596_Boat_Pose_or_Paripurna_Navasana_-a_stageii.npy')\n",
    "data  = np.load('/coc/scratch/sanisetty3/music_motion/GTA/new_joints_smpl/000001.npy')\n",
    "print(data.shape)\n",
    "renderer.render(\n",
    "                    motion_vec= torch.Tensor(data),\n",
    "                    outdir=render_path,\n",
    "                    step=0,\n",
    "                    name=\"rnd2\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "796cb92a-1eb0-48a1-9cf2-c5e163a33156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/coc/scratch/sanisetty3/music_motion/TGM3D'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f79d9-bbfd-4149-b06d-fe5d56394e95",
   "metadata": {},
   "source": [
    "## Trans eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7e0ca34-8981-4087-9925-82c83d06b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_processing.hml_process import HMLProcess\n",
    "from utils.motion_processing.aist_process import AISTProcess\n",
    "from utils.eval_trans import calculate_R_precision, calculate_activation_statistics, calculate_diversity, calculate_frechet_distance\n",
    "from core.datasets import dataset_TM_eval\n",
    "from core.models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "from utils.word_vectorizer import WordVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c8ce784-ce9a-42d8-ad0d-ed31eec61c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hml_processor = HMLProcess(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e33f9329-5612-47c5-a317-322b826c9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aist_processor = AISTProcess(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57005a14-73cd-4d79-9bff-52c6c9fa12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model = vqvae_model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f026d37b-d30c-46b0-b2d3-6971eb23a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4384/4384 [00:06<00:00, 711.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4248 4248\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w_vectorizer = WordVectorizer(\n",
    "    \"/srv/hays-lab/scratch/sanisetty3/music_motion/T2M-GPT/glove\", \"our_vab\"\n",
    ")\n",
    "eval_wrapper = EvaluatorModelWrapper(cfg.eval_model)\n",
    "tm_eval = dataset_TM_eval.DATALoader(\n",
    "    1,\n",
    "    w_vectorizer,\n",
    "    unit_length=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4bd4e8c0-9960-4b2b-8d81-01cc46290d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_sample = 0\n",
    "\n",
    "mean_gpt = np.load(\n",
    "    \"/srv/hays-lab/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/mean.npy\"\n",
    ")\n",
    "std_gpt = np.load(\n",
    "    \"/srv/hays-lab/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/std.npy\"\n",
    ")\n",
    "\n",
    "motion_annotation_list = []\n",
    "motion_pred_list = []\n",
    "\n",
    "R_precision_real = 0\n",
    "R_precision = 0\n",
    "\n",
    "nb_sample = 0\n",
    "matching_score_real = 0\n",
    "matching_score_pred = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f0150be-deb2-482a-9a3f-b571aa44598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/new_joint_vecs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f8881575-5419-4038-9fb3-121c7acff1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                             | 0/4248 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(tm_eval, position=0, leave=True):\n",
    "    (\n",
    "        word_embeddings,\n",
    "        pos_one_hots,\n",
    "        caption,\n",
    "        sent_len,\n",
    "        hml_motion,\n",
    "        m_length,\n",
    "        token,\n",
    "        name,\n",
    "    ) = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4c1e575f-2e1f-4808-bbc3-9bd7b1bd6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_mean = np.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/Mean.npy\")\n",
    "smpl_std = np.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/HumanMotionSMPL/HumanML3D_SMPL/Std.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a56a3925-0f79-4b54-b7b2-756d0a28cf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(60)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_length[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8c17fc8f-dcb4-4772-b6cb-9a9d4c168779",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = hml_motion.squeeze().to(float)[:m_length[0]]\n",
    "denorm = tm_eval.dataset.inv_transform(motion.detach().cpu())\n",
    "denorm2  = (denorm - mean_gpt) / std_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f51169f-1cc9-4100-b734-89ffabb28902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"M\" in name[0]:\n",
    "    smpl = np.load(os.path.join(dir , f\"180_{n[1:]}.npy\"))\n",
    "    smpl_motions = (aist_processor.aist6d_to_hml(smpl[:m_length[0]+1 , :135]))\n",
    "else:\n",
    "    smpl = np.load(os.path.join(dir , f\"00_{n}.npy\"))\n",
    "    smpl_motions = (aist_processor.aist6d_to_hml(smpl[:m_length[0] +1, :135]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5e60e02e-0433-4778-971b-9b44b13ae085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smpl_denorm =  ((smpl_motions - smpl_motions.mean(-1 , keepdims = True))/smpl_motions.std(-1, keepdims = True)) * denorm.std(-1, keepdims = True).numpy() + denorm.mean(-1, keepdims = True).numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c7857-3275-4903-8a25-9ff509ca63bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smpl_motions-smpl_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd62ee-a3cf-40ea-aac3-7ac2070e956a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96de86-84c8-455d-adee-b831b7f4df94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "086550be-e336-4396-8e68-aa95ffdc5b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ((denorm[0] - denorm[0].mean())/denorm[0].std())\n",
    "# denorm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "14ec406d-f803-4745-9a2b-fd12e30607cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((smpl_motions[0] - smpl_motions[0].mean())/smpl_motions[0].std()) * denorm[0].std().numpy() + denorm[0].mean().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6de3dd-798d-40cd-98aa-f5228d5683e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eea958c8-1afa-4002-9d9e-eb7b65a36280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# motion = motion.to(device)\n",
    "et, em = eval_wrapper.get_co_embeddings(\n",
    "    word_embeddings, pos_one_hots, sent_len, denorm, m_length\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0761598a-e080-4cd1-a6a6-a514b68a9b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mu, gt_cov = calculate_activation_statistics(em.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f06a4dec-7ecf-4ec6-971c-884090511465",
   "metadata": {},
   "outputs": [],
   "source": [
    "hml_data = []\n",
    "pred_pose_eval = torch.zeros((bs, seq, motion.shape[-1])).cuda()\n",
    "for i in range(len(smpl_motions)):\n",
    "    motion_aa = hml_processor.to_aist_axis_angle_from6d(torch.Tensor(smpl_motions[i][:,:135]))\n",
    "    pose_seq, body = hml_processor.aa2joints(motion_aa)\n",
    "    data_hml, rec_ric_data = hml_processor.get_hml_rep(pose_seq[:,:22].cpu().numpy())\n",
    "    pred_denorm = (data_hml - mean_gpt) / std_gpt\n",
    "    pred_pose_eval[i : i + 1, : pred_denorm.shape[0], :] = torch.Tensor(pred_denorm)\n",
    "    # hml_data.append(pred_denorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2269b74b-260f-4aac-9361-44bbf8d87d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 196, 263])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pose_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57ae0e1e-012f-4fad-b8f2-3a5dc29f8592",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_pred, em_pred = eval_wrapper.get_co_embeddings(\n",
    "    word_embeddings, pos_one_hots, sent_len, pred_pose_eval, m_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a285dba2-a855-4908-aad3-550dcffd0465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mu, cov = calculate_activation_statistics(em_pred[:27].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d741c1c-380d-4244-b030-16477b5a4701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfa4138a-236b-4875-b8e1-1b3ad2a3a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = calculate_frechet_distance(gt_mu, gt_cov, mu, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "845836e3-9d9e-4753-8828-e9642d86a2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.547529978650815"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd4a6c3-6741-49ae-8fc8-01378842a10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d3c56-3c86-448b-9513-ce189b4fd405",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, seq = motion.shape[0], motion.shape[1]\n",
    "\n",
    "pred_pose_eval = torch.zeros((bs, seq, motion.shape[-1])).cuda()\n",
    "\n",
    "for i in range(bs):\n",
    "    pred_pose, ind, loss_commit = net(motion[i : i + 1, : m_length[i]])\n",
    "    pred_denorm = val_loader.dataset.inv_transform(pred_pose.detach().cpu())\n",
    "    pred_denorm = (pred_denorm - mean_gpt) / std_gpt\n",
    "\n",
    "    pred_pose_eval[i : i + 1, : m_length[i], :] = pred_denorm\n",
    "\n",
    "et_pred, em_pred = eval_wrapper.get_co_embeddings(\n",
    "    word_embeddings, pos_one_hots, sent_len, pred_pose_eval, m_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6133baa-e892-4d20-ab9b-968a8fece199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa216c1a-d22c-4e2d-972d-bf5d4ab6cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for batch in tqdm(val_loader, position=0, leave=True):\n",
    "    (\n",
    "        word_embeddings,\n",
    "        pos_one_hots,\n",
    "        caption,\n",
    "        sent_len,\n",
    "        motion,\n",
    "        m_length,\n",
    "        token,\n",
    "        name,\n",
    "    ) = batch\n",
    "    motion = motion.to(torch.float32)\n",
    "    denorm = val_loader.dataset.inv_transform(motion.detach().cpu())\n",
    "    denorm = (denorm - mean_gpt) / std_gpt\n",
    "\n",
    "    motion = motion.cuda()\n",
    "    et, em = eval_wrapper.get_co_embeddings(\n",
    "        word_embeddings, pos_one_hots, sent_len, denorm, m_length\n",
    "    )\n",
    "    bs, seq = motion.shape[0], motion.shape[1]\n",
    "\n",
    "    num_joints = 21 if motion.shape[-1] == 251 else 22\n",
    "\n",
    "    pred_pose_eval = torch.zeros((bs, seq, motion.shape[-1])).cuda()\n",
    "\n",
    "    for i in range(bs):\n",
    "        pred_pose, ind, loss_commit = net(motion[i : i + 1, : m_length[i]])\n",
    "        pred_denorm = val_loader.dataset.inv_transform(pred_pose.detach().cpu())\n",
    "        pred_denorm = (pred_denorm - mean_gpt) / std_gpt\n",
    "\n",
    "        pred_pose_eval[i : i + 1, : m_length[i], :] = pred_denorm\n",
    "\n",
    "    et_pred, em_pred = eval_wrapper.get_co_embeddings(\n",
    "        word_embeddings, pos_one_hots, sent_len, pred_pose_eval, m_length\n",
    "    )\n",
    "\n",
    "    motion_pred_list.append(em_pred)\n",
    "    motion_annotation_list.append(em)\n",
    "\n",
    "    temp_R, temp_match = calculate_R_precision(\n",
    "        et.cpu().numpy(), em.cpu().numpy(), top_k=3, sum_all=True\n",
    "    )\n",
    "    R_precision_real += temp_R\n",
    "    matching_score_real += temp_match\n",
    "    temp_R, temp_match = calculate_R_precision(\n",
    "        et_pred.cpu().numpy(), em_pred.cpu().numpy(), top_k=3, sum_all=True\n",
    "    )\n",
    "    R_precision += temp_R\n",
    "    matching_score_pred += temp_match\n",
    "\n",
    "    nb_sample += bs\n",
    "\n",
    "motion_annotation_np = torch.cat(motion_annotation_list, dim=0).cpu().numpy()\n",
    "motion_pred_np = torch.cat(motion_pred_list, dim=0).cpu().numpy()\n",
    "gt_mu, gt_cov = calculate_activation_statistics(motion_annotation_np)\n",
    "mu, cov = calculate_activation_statistics(motion_pred_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7f36d-d00b-4d70-99e6-1b80e3fdf0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4b856-5496-45e7-906a-4a40f05bb968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626fa815-ca49-46dd-ad8a-5d59891c4a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b1d50-1237-43f9-a93d-55a73176fcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca0071-929c-47b3-8d8d-11c0e68d1010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5922ea-70d1-4c5e-ba3c-a6b1bfaa40fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f89b1a8d-5ae9-44bc-ac79-57fd64da7d77",
   "metadata": {},
   "source": [
    "## New LOss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e81db7-0f85-44b2-b6a7-424bfcf93411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.smpl.body_model import BodyModel\n",
    "import utils.rotation_conversions as geomtry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8923d4ee-ce9b-4ec5-9332-cd1ed28d14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = BodyModel(\"/srv/hays-lab/scratch/sanisetty3/music_motion/motion-diffusion-model/body_models/smplh/neutral/model.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcdf8770-33bf-4f3c-993d-94e5c46085f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in render_dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7502074-9f18-4998-a753-b67b633f17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m6d = batch[\"motion\"][:,:,:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6b106fa-084a-41c2-ac6e-3686fd9b62e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80, 135])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m6d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "524d6aaa-a437-4dde-9e8e-c9908aea49d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs , n , d = m6d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000aef29-320d-4b39-a1cc-05ebf7ae088e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82e12d8c-00f1-4934-ab2b-be18b8f151e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_trans = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "946e77cd-f0e0-4dc8-a8ab-d5e831c1bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = geomtry.matrix_to_axis_angle(geomtry.rotation_6d_to_matrix(m6d[:,:,3:135].reshape(-1,22,6))).reshape(-1 , 66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d44c1c36-4543-43fd-9c44-d5dbf2e35618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([160, 66])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f5ff9-8f29-4657-9652-c45fab766b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "40adbe11-b8cc-420a-b476-d21b9183c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = bm.forward(root_orient=aa[:,:3] , pose_body = aa[:,3:] , trans=m6d[:,:,:3].reshape(-1,3) , return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ecd0d94-8066-4b00-b1c0-5f94c1b7a5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['v', 'f', 'Jtr', 'full_pose'])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0df86be4-520d-46e0-a296-eff33916aa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80, 52, 3])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body[\"Jtr\"].reshape(bs , n , 52,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "70351076-370a-422b-bb19-c9538e2b619c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([159, 52, 3])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(body[\"Jtr\"][1:] - body[\"Jtr\"][:-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8958d3e8-4828-417d-a3ab-2b94ac56a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss = torch.nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "94e6b1d9-e664-437e-9cc0-87fe0ba6e491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loss(body[\"Jtr\"] , body[\"Jtr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0eeed11b-c977-4528-afdc-ad2375a4053b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80, 271])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m6d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d5de0d9-e9b8-479c-a326-05c724dccf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_new(motion_pred, motion_gt, mask=None):\n",
    "    Loss = torch.nn.SmoothL1Loss()\n",
    "    bs, n, d = motion_gt.shape  ## d = 135\n",
    "    motion_pred_aa = geomtry.matrix_to_axis_angle(\n",
    "        geomtry.rotation_6d_to_matrix(motion_pred[:, :, 3:135].reshape(-1, 22, 6))\n",
    "    ).reshape(-1, 66)\n",
    "    body_pred = bm.forward(\n",
    "        root_orient=motion_pred_aa[:, :3],\n",
    "        pose_body=motion_pred_aa[:, 3:],\n",
    "        trans=motion_pred[:, :, :3].reshape(-1, 3),\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "    motion_gt_aa = geomtry.matrix_to_axis_angle(\n",
    "        geomtry.rotation_6d_to_matrix(motion_gt[:, :, 3:135].reshape(-1, 22, 6))\n",
    "    ).reshape(-1, 66)\n",
    "    body_gt = bm.forward(\n",
    "        root_orient=motion_gt_aa[:, :3],\n",
    "        pose_body=motion_gt_aa[:, 3:],\n",
    "        trans=motion_gt[:, :, :3].reshape(-1, 3),\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "    j_pred = body_pred[\"Jtr\"].reshape(bs, n, 52, 3)[:, : 22, :]\n",
    "    j_gt = body_gt[\"Jtr\"].reshape(bs, n, 52, 3)[:, : 22, :]\n",
    "\n",
    "    loss_pos = Loss(\n",
    "        j_pred,\n",
    "        j_gt,\n",
    "    )\n",
    "    loss_vel = Loss(\n",
    "        (j_pred[:, 1:] - j_pred[:, :-1]), (j_gt[:, 1:] - j_gt[:, :-1])\n",
    "    )\n",
    "\n",
    "    return loss_pos, loss_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c7b1c73-6af9-420a-8fa4-35591dd35ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_pred, _ , _ = vqvae_model(m6d[:, :, :135])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a13cc6e-7e12-4973-ab54-7fffc95dda76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80, 135])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aac029e0-eeed-412c-9781-03266656662d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1199, grad_fn=<SmoothL1LossBackward0>),\n",
       " tensor(0.0787, grad_fn=<SmoothL1LossBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_new(motion_pred ,m6d )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05fcdea5-e8f3-4982-bf51-f6b52822090f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.42843907e-02,  1.13441283e+00, -8.98784186e-03, -2.40984987e-03,\n",
       "       -6.52230864e-04, -9.92232620e-04, -6.47345805e-03,  9.72569679e-01,\n",
       "        1.95284334e-02,  9.65837892e-01, -1.06665792e-01, -2.10577656e-02,\n",
       "        1.12139878e-01,  8.77811686e-01,  2.35941030e-01,  9.67205206e-01,\n",
       "        8.93590907e-02,  2.49379556e-02, -9.78139769e-02,  8.75245933e-01,\n",
       "        2.57273114e-01,  9.84242926e-01, -1.86104024e-02,  8.78357464e-03,\n",
       "        1.86940623e-02,  9.51855687e-01, -1.40547394e-01,  9.64632180e-01,\n",
       "        6.87693139e-02, -4.39669236e-02, -6.46595745e-02,  7.50103124e-01,\n",
       "       -4.13668740e-01,  9.68300201e-01, -5.46530094e-02,  8.35328890e-03,\n",
       "        4.05742119e-02,  7.39468683e-01, -4.42177031e-01,  9.91846624e-01,\n",
       "       -2.63669251e-03, -1.29083900e-02,  2.03791298e-03,  9.82947577e-01,\n",
       "       -6.16558589e-02,  9.84936983e-01, -4.41092218e-04,  6.37313024e-02,\n",
       "       -1.02082998e-02,  9.64401329e-01,  1.14246273e-01,  9.86655531e-01,\n",
       "       -2.35986068e-02, -5.49478645e-02,  3.18845415e-02,  9.64837748e-01,\n",
       "        1.07664388e-01,  9.95877394e-01,  1.04060404e-02,  4.32560868e-03,\n",
       "       -1.04378221e-02,  9.88724033e-01, -3.73103826e-02,  9.98310672e-01,\n",
       "       -6.12182906e-03,  5.28068073e-03,  5.02761189e-03,  9.97152197e-01,\n",
       "        8.36851638e-03,  9.97071286e-01,  7.25011364e-03,  3.04414303e-03,\n",
       "       -7.42382852e-03,  9.96662328e-01,  6.77212252e-04,  9.70232778e-01,\n",
       "        9.14622006e-03,  5.42326719e-03, -1.16296436e-02,  9.73011424e-01,\n",
       "       -3.50353014e-02,  9.21972276e-01,  1.28319900e-01, -6.45643217e-02,\n",
       "       -1.20774008e-01,  9.51249759e-01,  2.24948103e-02,  9.18653932e-01,\n",
       "       -1.07163985e-01,  1.05057156e-01,  1.01283654e-01,  9.51878612e-01,\n",
       "        3.23987403e-02,  9.85577040e-01, -1.01251840e-02,  2.29271893e-03,\n",
       "        8.80151979e-03,  9.73926622e-01,  1.24900396e-02,  6.26414120e-01,\n",
       "        5.63514330e-01, -2.37688198e-01, -5.40599279e-01,  6.73453504e-01,\n",
       "        8.01574207e-02,  6.37091834e-01, -5.48648146e-01,  2.40276587e-01,\n",
       "        5.04014381e-01,  6.86635257e-01,  1.32857638e-01,  4.79215240e-01,\n",
       "       -4.85518128e-02, -5.24101036e-01,  4.94350531e-02,  8.64559209e-01,\n",
       "       -3.68378402e-02,  4.57128376e-01,  3.07083826e-02,  5.39087418e-01,\n",
       "       -8.11344916e-02,  8.65124037e-01,  9.13718833e-03,  9.13432982e-01,\n",
       "       -3.58531085e-02, -3.69453652e-02,  3.63194239e-02,  8.88084743e-01,\n",
       "       -1.97448431e-02,  9.20908617e-01,  3.26237721e-02,  3.07632488e-02,\n",
       "       -3.58863521e-02,  8.87982305e-01,  3.40894737e-03])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"../HumanMotionSMPL/Mean.npy\")["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135fd5a-48f0-4400-8e54-74ee9b30f17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a28510f-681d-4923-8ec8-5d98ddc86d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/coc/scratch/sanisetty3/music_motion/TGM3D'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39ecd7b8-8dda-4a06-836a-c43788142fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/PyMAF-X\n"
     ]
    }
   ],
   "source": [
    "cd PyMAF-X/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dedc07-2d98-4b81-bee9-4df38cbf7e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8298abf-b95b-441e-aa70-0d9c2dff5f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c278d73-6c67-4376-81ac-3fe4a57289fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5043cf-d65f-4d70-adfc-aab0d894d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m apps.demo_smplx --image_folder examples/coco_images --detection_threshold 0.3 --pretrained_model data/pretrained_model/PyMAF-X_model_checkpoint_v1.1.pt --misc TRAIN.BHF_MODE full_body MODEL.PyMAF.HAND_VIS_TH 0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
