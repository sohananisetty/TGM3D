{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc1bc2a-0711-4b8d-ad3b-80ae634b5ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b3bcac-0d66-43d5-87a0-00a46af4adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e5a5540-2afa-425e-9340-b44dad2cc614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share2/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, device, dtype, nn\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.file_utils import (\n",
    "    ModelOutput,\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    MaskedLMOutput,\n",
    "    MultipleChoiceModelOutput,\n",
    "    NextSentencePredictorOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import (\n",
    "    PreTrainedModel,\n",
    "    apply_chunking_to_forward,\n",
    "    find_pruneable_heads_and_indices,\n",
    "    prune_linear_layer,\n",
    ")\n",
    "from transformers.utils import logging\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f74a75c1-f8e8-4133-91a1-c3469b92a62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d645672-29b0-4f2e-9e76-33cc5c4b6f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|███████████████████████████████████████████| 570/570 [00:00<00:00, 78.4kB/s]\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214788b2-5d00-4436-a376-4be701cbfafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ace3ef8-cdb9-4764-80bc-227f71a51814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74d0a83a-6bb0-46f9-94d1-8ce13cb8541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30862f19-b736-46e1-afec-3b263344325c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1996, 3007, 1997, 2605, 2003,  103, 1012,  102]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10110e5e-7472-4344-877d-4492c53e9989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02cca9be-42ea-4b85-a297-d7b46af5af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_proj = nn.Linear(model.config.hidden_size, 768) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b77ae1e-3706-466c-8c5d-1c6379c4ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_output = model.bert(inputs[\"input_ids\"], attention_mask = inputs[\"attention_mask\"],                      \n",
    "                                return_dict = True)            \n",
    "text_embeds = text_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d91656c-de4e-42b4-95c3-7650a69a56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feat = F.normalize(text_proj(text_embeds[:,0,:]),dim=-1)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "690c4b8f-5ba0-411d-a1d2-3dc3ff5f2140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6784a1b4-cf15-45e7-b15a-d88a9d6fb1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = (torch.zeros(1, 1, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "327b8cfe-d4eb-4a69-9ec7-88dd9b8bb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_tokens = cls_token.expand(4, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d129a2-eb1f-4948-9020-ef19bce518fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "127fe538-95f0-4bd7-a511-f4e5a594a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForSequenceClassification, T5EncoderModel, T5Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d55790d7-a8a3-4aee-98d4-f670209090f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = T5EncoderModel.from_pretrained(\"google/t5-v1_1-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a372faa9-bb39-48b4-af99-0cd210eb62a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eee1b8-4ef8-49e1-8292-0e0e53364aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab017e65-1215-4d73-8d4e-9328f816717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1864c01f-70ed-458f-9dc7-f4e115f45d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/srv/datasets/coco/person_keypoints_train2017.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"/srv/datasets/coco/captions_train2017.json\") as f:\n",
    "    data2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "38d8f1e5-c221-4c28-aacb-d06f153b90d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segmentation': [[267.03,\n",
       "   243.78,\n",
       "   314.59,\n",
       "   154.05,\n",
       "   357.84,\n",
       "   136.76,\n",
       "   374.05,\n",
       "   104.32,\n",
       "   410.81,\n",
       "   110.81,\n",
       "   429.19,\n",
       "   131.35,\n",
       "   420.54,\n",
       "   165.95,\n",
       "   451.89,\n",
       "   209.19,\n",
       "   464.86,\n",
       "   240.54,\n",
       "   480,\n",
       "   253.51,\n",
       "   484.32,\n",
       "   263.24,\n",
       "   496.22,\n",
       "   271.89,\n",
       "   484.32,\n",
       "   278.38,\n",
       "   438.92,\n",
       "   257.84,\n",
       "   401.08,\n",
       "   216.76,\n",
       "   370.81,\n",
       "   247.03,\n",
       "   414.05,\n",
       "   277.3,\n",
       "   433.51,\n",
       "   304.32,\n",
       "   443.24,\n",
       "   323.78,\n",
       "   400,\n",
       "   362.7,\n",
       "   376.22,\n",
       "   375.68,\n",
       "   400,\n",
       "   418.92,\n",
       "   394.59,\n",
       "   424.32,\n",
       "   337.3,\n",
       "   382.16,\n",
       "   337.3,\n",
       "   371.35,\n",
       "   388.11,\n",
       "   327.03,\n",
       "   341.62,\n",
       "   301.08,\n",
       "   311.35,\n",
       "   276.22,\n",
       "   304.86,\n",
       "   263.24,\n",
       "   294.05,\n",
       "   249.19]],\n",
       " 'num_keypoints': 8,\n",
       " 'area': 28292.08625,\n",
       " 'iscrowd': 0,\n",
       " 'keypoints': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  325,\n",
       "  160,\n",
       "  2,\n",
       "  398,\n",
       "  177,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  437,\n",
       "  238,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  477,\n",
       "  270,\n",
       "  2,\n",
       "  287,\n",
       "  255,\n",
       "  1,\n",
       "  339,\n",
       "  267,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  423,\n",
       "  314,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  355,\n",
       "  367,\n",
       "  2],\n",
       " 'image_id': 537548,\n",
       " 'bbox': [267.03, 104.32, 229.19, 320],\n",
       " 'category_id': 1,\n",
       " 'id': 183020}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"annotations\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b40accdf-f8c5-4f9e-a646-766aaf7fbaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 591753/591753 [00:00<00:00, 805192.43it/s]\n"
     ]
    }
   ],
   "source": [
    "ann1 = {}\n",
    "for dd in tqdm(data2[\"annotations\"]):\n",
    "    try:\n",
    "        ann1[dd[\"image_id\"]].append(dd[\"caption\"])\n",
    "    except:\n",
    "        ann1[dd[\"image_id\"]] = [dd[\"caption\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3fabd-a882-47be-84aa-bbb0ab787129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96992it [1:43:21, 15.45it/s]"
     ]
    }
   ],
   "source": [
    "persons = []\n",
    "for i, dat in tqdm(enumerate(data[\"images\"])):\n",
    "    d = {}\n",
    "    d['image_id'] = dat[\"id\"]\n",
    "    d[\"file_name\"] = data[\"images\"][i][\"file_name\"]\n",
    "    d[\"captions\"] = ann1[dat[\"id\"]]\n",
    "    d[\"coco_url\"] = dat[\"coco_url\"]\n",
    "    d[\"flickr_url\"] = dat[\"flickr_url\"]\n",
    "\n",
    "    is_crowd = [i[\"iscrowd\"] for i in data[\"annotations\"] if i[\"image_id\"] == d['image_id']]\n",
    "    if is_crowd == 0:\n",
    "        persons.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "598a2400-c310-47a0-a9d1-857028a9de8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118287"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cfde10df-3b6f-4623-b1c7-359a27fc82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = json.dumps(persons, indent=4)\n",
    " \n",
    "# Writing to sample.json\n",
    "with open(\"./coco_perons.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c8f6305c-ebad-402b-bcd2-ec22493f7934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/coc/scratch/sanisetty3/music_motion/TGM3D'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/coc/scratch/sanisetty3/music_motion/TGM3D/coco_perons.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1d684b0e-ddc8-4d20-ba8b-a7475d81b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5a365d5-6581-4acf-bfad-ed8acefca472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                         | 406/118287 [00:27<2:10:53, 15.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dd \u001b[38;5;129;01min\u001b[39;00m tqdm(persons):\n\u001b[1;32m      3\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/srv/datasets/coco/train2017/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m )\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdd\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/PIL/Image.py:2442\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   2441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n\u001b[0;32m-> 2442\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_path = \"/srv/hays-lab/scratch/sanisetty3/PyMAF-X/images/coco/\"\n",
    "for dd in tqdm(persons):\n",
    "    img = Image.open(f\"/srv/datasets/coco/train2017/{dd['file_name']}\" )\n",
    "    img.save(save_path + dd[\"file_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf4351-f2f5-4dfa-9556-eec8bbc75cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/srv/datasets/coco/train2017/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9e5d5fb-786b-4e12-8cff-50c4d655dd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5695fe28-3a51-4e6b-9406-382ebf70800a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12c626-e359-440f-b11e-e6add8a837e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
